{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e49b10-6fa2-4a93-858f-9fbf05c87dc7",
   "metadata": {},
   "source": [
    "# 3. Fetching .json Files for Retracted Papers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f73ab-ab81-4126-b519-52f55fc29964",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74035f-d7f9-4a25-b140-e31aef5ced92",
   "metadata": {},
   "source": [
    "\n",
    "This notebook **retrieves all the information available for our retracted papers from OpenAlex**. It does so by performing an API call. \n",
    "\n",
    "The Notebook takes the .csv file generated by **Notebook 2b**, which contained the cleaned dataset from the Retraction Watch Database, limited to a single discipline. It uses the DOI for all papers in that .csv file to perform the API call, then stores all the information available in .json files. These .json files will be used in **Notebook 4** to obtain abstracts for all of the retracted papers under investigation. These abstracts will in turn be used as input to train our model in **Notebook 6**.\n",
    "\n",
    "The **input and output parameters** for this Notebook uses are therefore as follows:\n",
    "\n",
    "-Input: **one csv file** with the data for all retracted papers within a specific field.\n",
    "-Output: **one json files** for each paper in our input file, with all the information available on OpenAlex for the paper in question.\n",
    "\n",
    "Please note that the output for this notebook takes up a lot of space. For this reason, its output path is set to an **external hardrive**. The path will therefore have to be adjusted when ran in a different machine.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d8f04-1a2a-40bd-b8f4-1f682aacacfa",
   "metadata": {},
   "source": [
    "## Input / Output Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf342ef3-9c36-47e2-ad65-b3b9b27defd8",
   "metadata": {},
   "source": [
    "- Input paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f957368d-4bf7-428d-a544-e57af461bc17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path for input .csv file\n",
    "\n",
    "input_path = \"../data/disciplines/BLS_Biology__Cellular.csv\"   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7507e5-1684-4c58-84d8-248641ef8568",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Output parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "258de979-a9c0-457b-8faa-751f77d60c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path for output .json files\n",
    "\n",
    "json_directory = \"/Volumes/Hurricane/cellbiology_retracted_fulljsonfiles\"\n",
    "\n",
    "# Path for log with information about downloaded items via API\n",
    "\n",
    "log_directory = \"../data/retracted_papers_json_APIcall_logs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40e893-9a23-4066-b4de-0c0475be25d5",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3277b86-04b5-4da3-9ec1-80cb7b2349c5",
   "metadata": {},
   "source": [
    "- Let us start by importing all the libraries that we will use in the Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d70c050-65e6-4ac5-a37f-6695ff022c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from json.decoder import JSONDecodeError\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7364f-849e-4030-84ce-39c9fbe736eb",
   "metadata": {},
   "source": [
    "## Loading Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b396c3-a677-4de6-a240-9e4a8f763337",
   "metadata": {},
   "source": [
    "- Next we will load our input .csv file into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c6878e4-c158-4fef-882d-80ccc54251b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>title</th>\n",
       "      <th>institution</th>\n",
       "      <th>journal</th>\n",
       "      <th>publisher</th>\n",
       "      <th>country</th>\n",
       "      <th>author</th>\n",
       "      <th>urls</th>\n",
       "      <th>article_type</th>\n",
       "      <th>retraction_date</th>\n",
       "      <th>...</th>\n",
       "      <th>original_paper_date</th>\n",
       "      <th>original_paper_doi</th>\n",
       "      <th>original_paper_pubmed_id</th>\n",
       "      <th>reason</th>\n",
       "      <th>paywalled</th>\n",
       "      <th>notes</th>\n",
       "      <th>year</th>\n",
       "      <th>reason_list</th>\n",
       "      <th>severity_score</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52739</td>\n",
       "      <td>Anti-breast Cancer Activity of Co(II) Complex ...</td>\n",
       "      <td>Luohe Medical College, Luohe, Henan, China; Lu...</td>\n",
       "      <td>Journal of Cluster Science</td>\n",
       "      <td>Springer - Nature Publishing Group</td>\n",
       "      <td>China</td>\n",
       "      <td>Ting Yin;Ruirui Wang;Shaozhe Yang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Research Article;</td>\n",
       "      <td>1/24/2024 0:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>10.1007/s10876-021-02192-4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>+Concerns/Issues About Image;+Concerns/Issues ...</td>\n",
       "      <td>No</td>\n",
       "      <td>See also: https://pubpeer.com/publications/739...</td>\n",
       "      <td>2021</td>\n",
       "      <td>['Concerns/Issues About Image', 'Concerns/Issu...</td>\n",
       "      <td>4</td>\n",
       "      <td>(BLS) Biology - Cellular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   record_id                                              title  \\\n",
       "0      52739  Anti-breast Cancer Activity of Co(II) Complex ...   \n",
       "\n",
       "                                         institution  \\\n",
       "0  Luohe Medical College, Luohe, Henan, China; Lu...   \n",
       "\n",
       "                      journal                           publisher country  \\\n",
       "0  Journal of Cluster Science  Springer - Nature Publishing Group   China   \n",
       "\n",
       "                              author urls       article_type retraction_date  \\\n",
       "0  Ting Yin;Ruirui Wang;Shaozhe Yang  NaN  Research Article;  1/24/2024 0:00   \n",
       "\n",
       "   ... original_paper_date          original_paper_doi  \\\n",
       "0  ...          2021-10-27  10.1007/s10876-021-02192-4   \n",
       "\n",
       "  original_paper_pubmed_id                                             reason  \\\n",
       "0                      0.0  +Concerns/Issues About Image;+Concerns/Issues ...   \n",
       "\n",
       "   paywalled                                              notes  year  \\\n",
       "0         No  See also: https://pubpeer.com/publications/739...  2021   \n",
       "\n",
       "                                         reason_list  severity_score  \\\n",
       "0  ['Concerns/Issues About Image', 'Concerns/Issu...               4   \n",
       "\n",
       "                    subject  \n",
       "0  (BLS) Biology - Cellular  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load input .csv data into data frame  \n",
    "\n",
    "discipline_df = pd.read_csv(input_path, encoding='latin-1')\n",
    "\n",
    "# Visualize data frame\n",
    "\n",
    "discipline_df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f4eb0d-7229-45ea-8372-71996b1bbcbe",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682cba65-a5ca-4075-9da1-49de46fb4270",
   "metadata": {},
   "source": [
    "- We will use a few functions to fetch the required information for our papers in a quick and efficient way. First, note that our dataset uses DOIs as the main identifiers for each retracted paper. In order to perform an API call and retrive all the information that OpenAlex possesses for a given paper, however, we will need to use an URL that conforms to the specific standards of our database. Luckily, this URL can easily be generated for each paper from its DOI. We will do that by using the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb9caae-d1d5-438e-b586-53fc11a0e28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define address_builder function\n",
    "\n",
    "def address_builder(doi):\n",
    "    \"\"\"Takes a DOI identifier and builds the full URL address to perofrm an API call\n",
    "    on OpenAlex from it\"\"\"\n",
    "    \n",
    "    # Build url address and store it in string   \n",
    "    \n",
    "    #base_address = \"https://api.openalex.org/works/https://doi.org/\" + doi\n",
    "    polite_address = base_address + \"?mailto=\" + \"pabloruizdeolano@gmail.com\" # Use polite address for faster API call performance\n",
    "    \n",
    "    # Return url address\n",
    "    \n",
    "    return polite_address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8aaa4b-3d97-42a6-9b48-47626d481c40",
   "metadata": {},
   "source": [
    "- Because we will be retreieving information for many retracted papers, and because API calls are often slow, downloading all the information that we are interested in will take a considerable amount of time. It will therefore be convenient to devise a system with which we can keep track of what items have been already downloaded and resume the process from there in case any interrumptions take place. We can do that by defining first the following function, which checks what are the DOIs of the papers for which the .json file with all the relevant data has already been downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c617a406-74bf-41c0-9fa1-5a672561a32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_downloaded_doi_dataframe(json_directory):\n",
    "    \n",
    "    # Create list with all downloaded files in directory\n",
    "    \n",
    "    json_files = [file for file in os.listdir(json_directory) if file.endswith('.json')]\n",
    "    \n",
    "    # Create list with DOIs of all downloaded files\n",
    "    \n",
    "    dois = [file[:-5].replace('_', '/') for file in json_files]\n",
    "    \n",
    "    # Create data frame with DOIs of all downloaded files\n",
    "    \n",
    "    doi_df = pd.DataFrame(dois, columns=['DOI'])\n",
    "    \n",
    "    # Return data frame\n",
    "    \n",
    "    return doi_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbafd0df-92bf-42b2-a95b-ced34441aa66",
   "metadata": {},
   "source": [
    "- We will also need a function that writes our data frame with the DOIs of the papers for which information has been downloaded into into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a19b9c0-d730-409e-ad4a-2af8d82b21f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define log_existing_dois function\n",
    "\n",
    "def log_existing_dois(existing_doi_df, log_directory):\n",
    "    \n",
    "    # Create file path for log\n",
    "    \n",
    "    log_file_path = os.path.join(log_directory, 'existing_doi_log.csv')\n",
    "    \n",
    "    # Save data frame with DOI of to .csv\n",
    "    \n",
    "    existing_doi_df.to_csv(log_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad233879-5c8c-4fcd-ac16-15303b9c0383",
   "metadata": {},
   "source": [
    "- And finally, we will define a function that we can use to generate a data frame with the DOIs of those papers for which no information has been downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ef66f67-fc50-4915-970a-58f55fa5369e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define filter_new_dois function\n",
    "\n",
    "def filter_new_dois(input_df, existing_doi_df):\n",
    "    \n",
    "    # Create data frame with DOIs of papers that have not been downloaded only\n",
    "    \n",
    "    filtered_df = input_df[~input_df['original_paper_doi'].isin(existing_doi_df['DOI'])]\n",
    "    \n",
    "    # Return data frame\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69c284-5c71-4cbc-96ea-27ae0c7cca85",
   "metadata": {},
   "source": [
    "- We can now proceed to define the functions that we will use to download information for our retracted papers. We will start by defining the main function that we will use to download the data of interest for retracted papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ba3e4e-aa8b-43a8-94fd-b19024829d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define API Call Function for each DOI\n",
    "\n",
    "def fetch_doi_fulljson(disciplines_doi_df, json_directory):\n",
    "    \n",
    "    # Create empty list to \n",
    "    \n",
    "    log = []\n",
    "    \n",
    "    # For loop to perform one API call per DOI in input data frame\n",
    "    \n",
    "    for doi in disciplines_doi_df['original_paper_doi']:\n",
    "        \n",
    "        # Build url address by calling address_builder function\n",
    "        \n",
    "        url = address_builder(doi)\n",
    "        \n",
    "        # Perform API call using URL address and store result in variable\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # If clause to control for case in which API call fails\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Convert result of API call to json format\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            # Create file path to create .json file with result of API call\n",
    "            \n",
    "            file_path = os.path.join(json_directory, doi.replace('/', '_') + '.json')\n",
    "            \n",
    "            # Save result of API call to .json file \n",
    "\n",
    "            with open(file_path, 'w') as file:\n",
    "                json.dump(data, file)\n",
    "            \n",
    "            # Update log list with dictionary specifying success for current DOI \n",
    "            \n",
    "            log.append({'DOI': doi, 'Status': 'Success'})\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Update log list with dictionary specifying failure for current DOI \n",
    "            \n",
    "            log.append({'DOI': doi, 'Status': f\"Failed - {response.status_code}\"})\n",
    "    \n",
    "    # Convert log list to data frame and return \n",
    "    \n",
    "    return pd.DataFrame(log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496cffb-6faf-4278-96f8-fa94eb8b605b",
   "metadata": {},
   "source": [
    "- We will also use a function to write the contet of the dataframe that we will use to keep track of what are the papers for which information has already been downloaded into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a006f79b-4932-47a0-a40a-49e72ec8d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define write_api_call_log\n",
    "\n",
    "def write_api_call_log(api_log_df, log_directory):\n",
    "    \n",
    "    # Create path to create .csv file from directory passed as input\n",
    "    \n",
    "    log_file_path = os.path.join(log_directory, 'doi_calling_log.csv')\n",
    "    \n",
    "    # Write content of data frame into resulting path\n",
    "    \n",
    "    api_log_df.to_csv(log_file_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eea1d0-3395-4adb-8751-ecb43c13111f",
   "metadata": {},
   "source": [
    "- Finally, we will create a function that calls the last two functions and thus attempts to download information for all relevant DOIs and logs the outcome of each attempt, all in one go:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f2fb60c-a13e-48cd-81f7-524e1276d468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define function to Run Api calls and log results\n",
    "\n",
    "def fetch_and_log_data(filtered_doi_df, json_directory, log_directory):\n",
    "    \n",
    "    # Fetch data for DOIs in data frame passed as input\n",
    "    \n",
    "    api_log_df = fetch_doi_fulljson(filtered_doi_df, json_directory)\n",
    "    \n",
    "    # Write log with result of API calls\n",
    "    \n",
    "    write_api_call_log(api_log_df, log_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df8a16-81e1-4d63-93e8-84aa652c2510",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811faec0-0948-494f-9c24-7d254dbf4523",
   "metadata": {},
   "source": [
    "\n",
    "- Having defined those definitions, we can go ahead and start downloading information for our retracted papers from OpenAlex. Let us start by calling one of the functions that we defined earlier to generate a data frame with the DOIs of all the papers that have already been downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce5a66b1-13c5-463b-a77e-8ca49b26d2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Volumes/Hurricane/cellbiology_retracted_fulljsonfiles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call function to generate data frame with DOIs of downloaded papers\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m existing_doi_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_downloaded_doi_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_directory\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m, in \u001b[0;36mcreate_downloaded_doi_dataframe\u001b[0;34m(json_directory)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_downloaded_doi_dataframe\u001b[39m(json_directory):\n\u001b[1;32m      2\u001b[0m     \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# List all JSON files and extract DOIs\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     json_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_directory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      6\u001b[0m     dois \u001b[38;5;241m=\u001b[39m [f[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m json_files]\n\u001b[1;32m      7\u001b[0m     doi_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dois, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/Hurricane/cellbiology_retracted_fulljsonfiles'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Call function to generate data frame with DOIs of downloaded papers\n",
    "\n",
    "existing_doi_df = create_downloaded_doi_dataframe(json_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0346336c-c902-4f82-a25b-1061d6b15df3",
   "metadata": {},
   "source": [
    "- Next we will save the information in this data frame into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5f440-4722-4a44-8c81-e05b7446480e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create .csv file with DOIs of papers for which information has already been downloaded\n",
    "\n",
    "log_existing_dois(existing_doi_df, log_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4885e36-5c68-45dc-88af-54c2ecede0c1",
   "metadata": {},
   "source": [
    "- Having done this, we can now create a data frame that only contains the DOIs of those retracted papers for which no information has been yet downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aaaba851-352b-42b8-83e8-ae5e1e96d421",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'existing_doi_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m     filtered_df \u001b[38;5;241m=\u001b[39m input_df[\u001b[38;5;241m~\u001b[39minput_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_paper_doi\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(existing_doi_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDOI\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_df\n\u001b[0;32m----> 8\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m filter_new_dois(discipline_df, \u001b[43mexisting_doi_df\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'existing_doi_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create data frame with DOIs of papers for which no data has been downloaded\n",
    "\n",
    "filtered_df = filter_new_dois(discipline_df, existing_doi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bca16a-a1d9-419c-b66e-4f6cfec58760",
   "metadata": {},
   "source": [
    "\n",
    "- We can now use the rest of the functions that we defined to start performing API calls to download information for our retracted papers from OpenAlex. Since data sets of interest will typically be quite large, we will first do that on a smaller sample, just to make sure that everything works properly. Let us first generate the required sample data size, with some desired sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2fa6078c-6185-40c1-b180-d3e1b218dc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 1 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   original_paper_doi  20 non-null     object\n",
      "dtypes: object(1)\n",
      "memory usage: 292.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Define sample size\n",
    "\n",
    "sample_size = 20\n",
    "\n",
    "# Check if sample_size is less than the number of rows in the data frame\n",
    "\n",
    "if sample_size <= len(discipline_df):\n",
    "    \n",
    "    # Create a random sample of the data frame with the defined sample size\n",
    "    \n",
    "    sample_df = discipline_df.sample(n=sample_size, random_state=1)  \n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"Sample size is larger than the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0939a-c4e8-4062-b2aa-995cab1c53ca",
   "metadata": {},
   "source": [
    "\n",
    "- Let us now call our main function to download data and write the appropriate logs for our sample data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc14e99-180a-42b7-ae90-745bdbfd5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call fetch_and_log_data function to download data for sample data frame\n",
    "\n",
    "fetch_and_log_data(sample_df, json_directory, log_directory) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b50766-ec54-4f5f-85a3-5ecce6ccd96b",
   "metadata": {},
   "source": [
    "- We can now inspect the information that the content of the .json files that were created for our sample data frame, and the log files that were generated in the process. If it all looks good, we can go ahead and download information for the rest of our retracted papers by using the functions that we defined earlier, which find out what are the papers for which information has already been downloaded and generate a new data frame that contains only the DOIs of those papers for which an API call still remains to be performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910550d-ad23-463c-85a3-cbd5de9019ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call function to write .csv file with DOIs of papers for which we already have information\n",
    "\n",
    "log_existing_dois(existing_doi_df, log_directory)\n",
    "\n",
    "# Call function to generate data frame from .csv file produced by previous function call\n",
    "\n",
    "filtered_df = filter_new_dois(discipline_df, existing_doi_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2e1ab-3187-4a64-a0b1-40779b4667cc",
   "metadata": {},
   "source": [
    "\n",
    "- Having found out what are the papers for which information still has to be downloaded, we can go on to perform the appropriate API calls for the rest of our retracted papers. With this, we will have obtained all the information that we wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f367f77-e058-47a1-8f4c-1ca5a606a7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fetch all json files for the main corpus of the discipline \n",
    "\n",
    "fetch_and_log_data(filtered_df, json_directory, log_directory) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
