{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e49b10-6fa2-4a93-858f-9fbf05c87dc7",
   "metadata": {},
   "source": [
    "# 2. Fetching .json Files for Retracted Papers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f73ab-ab81-4126-b519-52f55fc29964",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74035f-d7f9-4a25-b140-e31aef5ced92",
   "metadata": {},
   "source": [
    "\n",
    "This notebook **retrieves all the information available for our retracted papers from OpenAlex**. It does so by performing an API call from OpenAlex.\n",
    "\n",
    "The Notebook takes the .csv file generated by **Notebook 1c**, which contained the cleaned dataset which included a severity score for each paper and was limited to a single discipline. It then uses the DOI for all papers in that .csv file to perform the API call and store all the information available in .json files. These .json files will be used in **Notebooks 2b and 2c** to obtain abstracts for all of the retracted papers under investigation, along with the exact distribution of these papers by country and year. \n",
    "\n",
    "The **worklflow** for this Notebook is therefore as follows:\n",
    "\n",
    "- Input: **one .csv file** with the our clean data for all retracted papers within a specific field.\n",
    "- Output: **one .json file for each paper in our input .csv file**, with all the information available on OpenAlex for the paper in question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d8f04-1a2a-40bd-b8f4-1f682aacacfa",
   "metadata": {},
   "source": [
    "## Input / Output Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf342ef3-9c36-47e2-ad65-b3b9b27defd8",
   "metadata": {},
   "source": [
    "Input paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f957368d-4bf7-428d-a544-e57af461bc17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path for input .csv file\n",
    "\n",
    "input_path = \"../data/subject_cell_bio\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7507e5-1684-4c58-84d8-248641ef8568",
   "metadata": {
    "tags": []
   },
   "source": [
    "Output parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "258de979-a9c0-457b-8faa-751f77d60c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path for output .json files\n",
    "\n",
    "#json_directory = \"/Volumes/TOSHIBA_EXT/cellbiology_retracted_fulljsonfiles\"\n",
    "\n",
    "json_directory = \"../data/json_files/cellbiology_retracted_fulljsonfiles\"\n",
    "\n",
    "# Path for log with information about downloaded of .json files progress\n",
    "\n",
    "log_directory = \"../data/logs/retracted_papers_json_APIcall_logs\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40e893-9a23-4066-b4de-0c0475be25d5",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3277b86-04b5-4da3-9ec1-80cb7b2349c5",
   "metadata": {},
   "source": [
    "Let us start by importing all the libraries that we will use in the Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d70c050-65e6-4ac5-a37f-6695ff022c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from json.decoder import JSONDecodeError\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7364f-849e-4030-84ce-39c9fbe736eb",
   "metadata": {},
   "source": [
    "## Loading Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b396c3-a677-4de6-a240-9e4a8f763337",
   "metadata": {},
   "source": [
    "Next we will load our input .csv file into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6878e4-c158-4fef-882d-80ccc54251b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>title</th>\n",
       "      <th>institution</th>\n",
       "      <th>journal</th>\n",
       "      <th>publisher</th>\n",
       "      <th>country</th>\n",
       "      <th>author</th>\n",
       "      <th>urls</th>\n",
       "      <th>article_type</th>\n",
       "      <th>retraction_date</th>\n",
       "      <th>...</th>\n",
       "      <th>original_paper_date</th>\n",
       "      <th>original_paper_doi</th>\n",
       "      <th>original_paper_pubmed_id</th>\n",
       "      <th>reason</th>\n",
       "      <th>paywalled</th>\n",
       "      <th>notes</th>\n",
       "      <th>year</th>\n",
       "      <th>reason_list</th>\n",
       "      <th>severity_score</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52739</td>\n",
       "      <td>Anti-breast Cancer Activity of Co(II) Complex ...</td>\n",
       "      <td>Luohe Medical College, Luohe, Henan, China; Lu...</td>\n",
       "      <td>Journal of Cluster Science</td>\n",
       "      <td>Springer - Nature Publishing Group</td>\n",
       "      <td>China</td>\n",
       "      <td>Ting Yin;Ruirui Wang;Shaozhe Yang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Research Article;</td>\n",
       "      <td>1/24/2024 0:00</td>\n",
       "      <td>...</td>\n",
       "      <td>2021-10-27</td>\n",
       "      <td>10.1007/s10876-021-02192-4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>+Concerns/Issues About Image;+Concerns/Issues ...</td>\n",
       "      <td>No</td>\n",
       "      <td>See also: https://pubpeer.com/publications/739...</td>\n",
       "      <td>2021</td>\n",
       "      <td>['Concerns/Issues About Image', 'Concerns/Issu...</td>\n",
       "      <td>4</td>\n",
       "      <td>(BLS) Biology - Cellular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   record_id                                              title  \\\n",
       "0      52739  Anti-breast Cancer Activity of Co(II) Complex ...   \n",
       "\n",
       "                                         institution  \\\n",
       "0  Luohe Medical College, Luohe, Henan, China; Lu...   \n",
       "\n",
       "                      journal                           publisher country  \\\n",
       "0  Journal of Cluster Science  Springer - Nature Publishing Group   China   \n",
       "\n",
       "                              author urls       article_type retraction_date  \\\n",
       "0  Ting Yin;Ruirui Wang;Shaozhe Yang  NaN  Research Article;  1/24/2024 0:00   \n",
       "\n",
       "   ... original_paper_date          original_paper_doi  \\\n",
       "0  ...          2021-10-27  10.1007/s10876-021-02192-4   \n",
       "\n",
       "  original_paper_pubmed_id                                             reason  \\\n",
       "0                      0.0  +Concerns/Issues About Image;+Concerns/Issues ...   \n",
       "\n",
       "   paywalled                                              notes  year  \\\n",
       "0         No  See also: https://pubpeer.com/publications/739...  2021   \n",
       "\n",
       "                                         reason_list  severity_score  \\\n",
       "0  ['Concerns/Issues About Image', 'Concerns/Issu...               4   \n",
       "\n",
       "                    subject  \n",
       "0  (BLS) Biology - Cellular  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load input .csv data into data frame  \n",
    "\n",
    "df = pd.read_csv(input_path, encoding='latin-1')\n",
    "\n",
    "# Visualize data frame\n",
    "\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f4eb0d-7229-45ea-8372-71996b1bbcbe",
   "metadata": {},
   "source": [
    "## Downloading Abstracts for Retracted Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682cba65-a5ca-4075-9da1-49de46fb4270",
   "metadata": {},
   "source": [
    "\n",
    "Our goal in this notebooks will be to download all the information that Open Alex has on the retracted papers that we are investigating. One can do that by making an API call to the OpenAlex database, using an URL that is specific to each paper. Luckily, this URL can easily be constructed from each papers DOI. \n",
    "\n",
    "Since we will need to use this in what follows, let us go ahead and define a function that constructs an OpenAlex-appropriate URL give a papers DOI:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb9caae-d1d5-438e-b586-53fc11a0e28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define address_builder function\n",
    "\n",
    "def address_builder(doi):\n",
    "    \"\"\"Takes a DOI identifier and builds the full URL address to perform an API call\n",
    "    on OpenAlex from it\"\"\"\n",
    "    \n",
    "    # Build url address and store it in string   \n",
    "    \n",
    "    base_address = \"https://api.openalex.org/works/https://doi.org/\" + doi\n",
    "    polite_address = base_address + \"?mailto=\" + \"pabloruizdeolano@gmail.com\" # Use polite address for faster API call performance\n",
    "    \n",
    "    # Return url address\n",
    "    \n",
    "    return polite_address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8aaa4b-3d97-42a6-9b48-47626d481c40",
   "metadata": {},
   "source": [
    "Because we will be retreieving information for many retracted papers, and because API calls are often slow, downloading all the information that we are interested in will take a considerable amount of time. It will therefore be convenient to devise a system with which we can keep track of what items have been already downloaded and resume the process from there in case any interrumptions take place. \n",
    "\n",
    "We can do that by defining first the following function, which checks what are the names of the files in our output directory, then reconstructs the DOIs of the papers for which we already have information, returns a data frame with this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c617a406-74bf-41c0-9fa1-5a672561a32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define function to get list of already downloaded DOIs\n",
    "\n",
    "def downloaded_doi_list_getter(directory):\n",
    "    \"\"\"\n",
    "    Function takes a file path as input, checks how many .json files there are in there,\n",
    "    then reconstructs the DOIs associated to each file from their names by removing the file\n",
    "    extension, returns a data frame with resulting DOIs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create list with names of all files in input directory\n",
    "    \n",
    "    file_names = [file for file in os.listdir(directory) if file.endswith('.json')]\n",
    "    \n",
    "    # Create list with names of all files minus \".json\" extension  \n",
    "    # Given the name structure of our files, this will give us a list of all DOIs in folder\n",
    "    \n",
    "    paper_dois = [file[:-5].replace('_', '/') for file in file_names]\n",
    "    \n",
    "    # Create data frame with names of all files in folder\n",
    "    \n",
    "    df_dois = pd.DataFrame(paper_dois, columns=['doi'])\n",
    "    \n",
    "    # Return data frame\n",
    "    \n",
    "    return df_dois\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbafd0df-92bf-42b2-a95b-ced34441aa66",
   "metadata": {},
   "source": [
    "\n",
    "For compactness, it will also be useful to define a function that writes the list of DOIs of papers for which we already have a .json file into a .csv file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a19b9c0-d730-409e-ad4a-2af8d82b21f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define function to print list of downloaded DOIs into .csv file\n",
    "\n",
    "\"\"\"\n",
    "Function takes a data frame and a file path as input, prints the content of the data frame\n",
    "as a .csv file in the directory that the file path indicates.\n",
    "\"\"\"\n",
    "\n",
    "def data_frame_printer(df, directory, file_name):\n",
    "    \n",
    "    # Create file path for log\n",
    "    \n",
    "    full_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    #full_path = os.path.join(directory, 'existing_doi_list.csv')\n",
    "\n",
    "    # Save data frame with DOI of to .csv\n",
    "    \n",
    "    df.to_csv(full_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad233879-5c8c-4fcd-ac16-15303b9c0383",
   "metadata": {},
   "source": [
    "\n",
    "And finally, let us also define a function that we can use to generate to remove the papers for which a .json file has already been downloaded from our original data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ef66f67-fc50-4915-970a-58f55fa5369e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define function to remove papers for which we already have a .json file from original data frame\n",
    "\n",
    "def filter_new_dois(df, existing_doi_df):\n",
    "    \"\"\"\n",
    "    Function takes an input data frame and a data frame with a list of DOIs, returns \n",
    "    the input data frame without those papers whose DOIs where included in the list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create data frame with DOIs of papers that have not been downloaded only\n",
    "    \n",
    "    df_ony_not_downloaded = df[~df['original_paper_doi'].isin(existing_doi_df['DOI'])]\n",
    "    \n",
    "    # Return data frame\n",
    "    \n",
    "    return df_ony_not_downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69c284-5c71-4cbc-96ea-27ae0c7cca85",
   "metadata": {},
   "source": [
    "\n",
    "We can now proceed to define the functions that we will use to download information for our retracted papers. We will start by defining the main function that we will use to download the data of interest for retracted papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68ba3e4e-aa8b-43a8-94fd-b19024829d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function to obtain .json files for all papers in our data frame\n",
    "\n",
    "\"\"\"\n",
    "Function takes a data frame and a file path to a directory, extracts a list of DOIs from\n",
    "the \"original_paper_doi\" column of input data frame and performs one API call per DOI, \n",
    "writes outcome as .json file in input directory. If also keeps a log of successful and\n",
    "failed API calls, returns log as data frame with list of DOIs with outcome of call.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def fetch_json_files(df, directory):\n",
    "    \n",
    "    # Create empty list to store log with success or failure of each API call\n",
    "    \n",
    "    log = []\n",
    "    \n",
    "    # For loop to perform one API call per DOI in input data frame\n",
    "    \n",
    "    for doi in df['original_paper_doi']:\n",
    "        \n",
    "        # Build url address by calling address_builder function\n",
    "        \n",
    "        url = address_builder(doi)\n",
    "        \n",
    "        # Perform API call using URL address and store result in variable\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # If clause to control for case in which API call fails\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            \n",
    "            # Convert result of API call to json format\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            # Create file path to save .json file with result of API call\n",
    "            \n",
    "            full_path = os.path.join(directory, doi.replace('/', '_') + '.json')\n",
    "            \n",
    "            # Save result of API call to .json file \n",
    "\n",
    "            with open(full_path, 'w') as file:\n",
    "                json.dump(data, file)\n",
    "            \n",
    "            # Update log list with dictionary specifying success for current DOI \n",
    "            \n",
    "            log.append({'DOI': doi, 'Status': 'Success'})\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Update log list with dictionary specifying failure for current DOI \n",
    "            \n",
    "            log.append({'DOI': doi, 'Status': f\"Failed - {response.status_code}\"})\n",
    "    \n",
    "    # Convert log list to data frame and return \n",
    "    \n",
    "    return pd.DataFrame(log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496cffb-6faf-4278-96f8-fa94eb8b605b",
   "metadata": {},
   "source": [
    "\n",
    "THIS CELL CAN BE DELETED BUT I'M KEEPING IT HERE TO KEEP A RECORD OF THE FILE NAMES USED ETC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a006f79b-4932-47a0-a40a-49e72ec8d1db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define write_api_call_log\n",
    "\n",
    "def write_api_call_log(api_log_df, log_directory):\n",
    "    \n",
    "    # Create path to create .csv file from directory passed as input\n",
    "    \n",
    "    log_file_path = os.path.join(log_directory, 'doi_calling_log.csv')\n",
    "    \n",
    "    # Write content of data frame into resulting path\n",
    "    \n",
    "    api_log_df.to_csv(log_file_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eea1d0-3395-4adb-8751-ecb43c13111f",
   "metadata": {},
   "source": [
    "Finally, we will create a function that calls the last two functions and thus attempts to download information for all relevant DOIs and logs the outcome of each attempt, all in one go:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f2fb60c-a13e-48cd-81f7-524e1276d468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define function to run Api calls and log results\n",
    "\n",
    "def fetch_and_log_data(filtered_doi_df, json_directory, log_directory):\n",
    "    \n",
    "    # Fetch data for DOIs in data frame passed as input\n",
    "    \n",
    "    api_log_df = fetch_json_files(filtered_doi_df, json_directory)\n",
    "    \n",
    "    # Write log with result of API calls\n",
    "    \n",
    "    write_api_call_log(api_log_df, log_directory)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df8a16-81e1-4d63-93e8-84aa652c2510",
   "metadata": {},
   "source": [
    "## First Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bca16a-a1d9-419c-b66e-4f6cfec58760",
   "metadata": {},
   "source": [
    "\n",
    "Having defined those functions, we can go ahead and start downloading information for our retracted papers from OpenAlex. Since data sets of interest will typically be quite large, we will first do that on a smaller sample, just to make sure that everything works properly. Let us first generate the required sample data size, with some desired sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fa6078c-6185-40c1-b180-d3e1b218dc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define sample size\n",
    "\n",
    "sample_size = 20\n",
    "\n",
    "# Check if sample_size is less than the number of rows in the data frame\n",
    "\n",
    "if sample_size <= len(discipline_df):\n",
    "    \n",
    "    # Create a random sample of the data frame with the defined sample size\n",
    "    \n",
    "    sample_df = discipline_df.sample(n=sample_size, random_state=1)  \n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"Sample size is larger than the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f0939a-c4e8-4062-b2aa-995cab1c53ca",
   "metadata": {},
   "source": [
    "\n",
    "We can now call our main function to download data and write the appropriate logs for our sample data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bc14e99-180a-42b7-ae90-745bdbfd5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call fetch_and_log_data function to download data for sample data frame\n",
    "\n",
    "fetch_and_log_data(sample_df, json_directory, log_directory) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b85a679-8757-4fe4-876b-a9b3d4fa5550",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Next we call one of the functions that we defined earlier to generate a data frame with the DOIs of all the papers that have already been downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce5a66b1-13c5-463b-a77e-8ca49b26d2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Call function to generate data frame with DOIs of downloaded papers\n",
    "\n",
    "existing_doi_df = downloaded_doi_list_getter(json_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7f5f4-ec5c-4110-a308-3f818ca2d85f",
   "metadata": {},
   "source": [
    "And we save the information in this data frame into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1f5f440-4722-4a44-8c81-e05b7446480e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create .csv file with DOIs of papers for which information has already been downloaded\n",
    "\n",
    "data_frame_printer(existing_doi_df, log_directory, \"existing_doi_list.csv\")\n",
    "#log_existing_dois(existing_doi_df, log_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960388b-7bbd-4c91-97ba-83d2757bf028",
   "metadata": {},
   "source": [
    "We can now inspect the information that the content of the .json files that were created for our sample data frame, and the log files that were generated in the process. If it all looks good, we can go ahead and download information for the rest of our retracted papers by using the functions that we defined earlier. \n",
    "\n",
    "We will start by creating a new data frame with the DOIs of all the papers of interest for which no data has yet been downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aaaba851-352b-42b8-83e8-ae5e1e96d421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create data frame with DOIs of papers for which no data has been downloaded\n",
    "\n",
    "filtered_df = filter_new_dois(discipline_df, existing_doi_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a444fc1-31f4-44d0-abb5-e5b10fe08712",
   "metadata": {},
   "source": [
    "\n",
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d2e1ab-3187-4a64-a0b1-40779b4667cc",
   "metadata": {},
   "source": [
    "\n",
    "With this information, we can now go on to perform the appropriate API calls for the rest of our retracted papers. Once we are done with that, we will have obtained all the information that we wanted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f367f77-e058-47a1-8f4c-1ca5a606a7f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Fetch all json files for the main corpus of the discipline \n",
    "\n",
    "fetch_and_log_data(filtered_df, json_directory, log_directory) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "047d5a73-2810-4ebf-889d-2f9e6c5d7ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7071, 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_doi_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d277e122-feb9-4ca4-824b-fa7a5e5d6cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228, 22)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
