{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e49b10-6fa2-4a93-858f-9fbf05c87dc7",
   "metadata": {},
   "source": [
    "# 2a. Downloading JSON Files for Retracted Papers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7f73ab-ab81-4126-b519-52f55fc29964",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74035f-d7f9-4a25-b140-e31aef5ced92",
   "metadata": {},
   "source": [
    "\n",
    "This notebook **retrieves all the information available for our sample of retracted papers from OpenAlex**. It does so by accessing this database via a series of API calls.\n",
    "\n",
    "The Notebook takes the .csv file generated by **Notebook 1c**, which contained our cleaned dataset, filtered by severity score and limited to a single discipline. It then uses the DOI for all papers in that .csv file to perform the API call to OpenAlex and store all the information available in the database in a number of .json files. These .json files will be used in **Notebooks 2b and 2c** to obtain abstracts for all of the retracted papers under investigation, along with the exact distribution of these papers by country and year. \n",
    "\n",
    "The **worklflow** for this Notebook is therefore as follows:\n",
    "\n",
    "- Input: **one .csv file** with the our clean data for all retracted papers within a specific field.\n",
    "- Output: **one .json file for each paper in our input .csv file**, **two .csv file** with logs concerning the download process and the points at which it may have failed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d8f04-1a2a-40bd-b8f4-1f682aacacfa",
   "metadata": {},
   "source": [
    "## Input / Output Parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf342ef3-9c36-47e2-ad65-b3b9b27defd8",
   "metadata": {},
   "source": [
    "Input paramters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f957368d-4bf7-428d-a544-e57af461bc17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path for input .csv file\n",
    "\n",
    "input_path = \"../data/retraction_watch_data_set/4_cell_bio_data_set.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7507e5-1684-4c58-84d8-248641ef8568",
   "metadata": {
    "tags": []
   },
   "source": [
    "Output parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "258de979-a9c0-457b-8faa-751f77d60c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Path to directory to store .json files\n",
    "\n",
    "#json_directory = \"/Volumes/TOSHIBA_EXT/cellbiology_retracted_fulljsonfiles\"\n",
    "\n",
    "json_directory = \"../data/json_files/cell_biology/retracted\"\n",
    "\n",
    "# Path for log with information about downloaded of .json files progress\n",
    "\n",
    "log_directory_api_outcome = \"../data/logs/api_call_outcomes.csv\"\n",
    "\n",
    "# Path for log with information about downloaded of .json files progress\n",
    "\n",
    "log_directory_doi_list = \"../data/logs/downloaded_doi_list.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40e893-9a23-4066-b4de-0c0475be25d5",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3277b86-04b5-4da3-9ec1-80cb7b2349c5",
   "metadata": {},
   "source": [
    "Let us start by importing all the libraries that we will use in the Notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0d70c050-65e6-4ac5-a37f-6695ff022c72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from json.decoder import JSONDecodeError\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7364f-849e-4030-84ce-39c9fbe736eb",
   "metadata": {},
   "source": [
    "## Loading Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b396c3-a677-4de6-a240-9e4a8f763337",
   "metadata": {},
   "source": [
    "Next we will load our input .csv file into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5c6878e4-c158-4fef-882d-80ccc54251b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>title</th>\n",
       "      <th>institution</th>\n",
       "      <th>journal</th>\n",
       "      <th>publisher</th>\n",
       "      <th>country</th>\n",
       "      <th>author</th>\n",
       "      <th>urls</th>\n",
       "      <th>article_type</th>\n",
       "      <th>retraction_date</th>\n",
       "      <th>...</th>\n",
       "      <th>original_paper_date</th>\n",
       "      <th>original_paper_doi</th>\n",
       "      <th>original_paper_pubmed_id</th>\n",
       "      <th>retraction_nature</th>\n",
       "      <th>reason</th>\n",
       "      <th>paywalled</th>\n",
       "      <th>notes</th>\n",
       "      <th>reason_list</th>\n",
       "      <th>severity_score</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52739</td>\n",
       "      <td>Anti-breast Cancer Activity of Co(II) Complex ...</td>\n",
       "      <td>Luohe Medical College, Luohe, Henan, China; Lu...</td>\n",
       "      <td>Journal of Cluster Science</td>\n",
       "      <td>Springer - Nature Publishing Group</td>\n",
       "      <td>China</td>\n",
       "      <td>Ting Yin;Ruirui Wang;Shaozhe Yang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Research Article;</td>\n",
       "      <td>1/24/2024 0:00</td>\n",
       "      <td>...</td>\n",
       "      <td>10/27/2021 0:00</td>\n",
       "      <td>10.1007/s10876-021-02192-4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Retraction</td>\n",
       "      <td>+Concerns/Issues About Image;+Concerns/Issues ...</td>\n",
       "      <td>No</td>\n",
       "      <td>See also: https://pubpeer.com/publications/739...</td>\n",
       "      <td>['Concerns/Issues About Image', 'Concerns/Issu...</td>\n",
       "      <td>4</td>\n",
       "      <td>(BLS) Biology - Cellular</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   record_id                                              title  \\\n",
       "0      52739  Anti-breast Cancer Activity of Co(II) Complex ...   \n",
       "\n",
       "                                         institution  \\\n",
       "0  Luohe Medical College, Luohe, Henan, China; Lu...   \n",
       "\n",
       "                      journal                           publisher country  \\\n",
       "0  Journal of Cluster Science  Springer - Nature Publishing Group   China   \n",
       "\n",
       "                              author urls       article_type retraction_date  \\\n",
       "0  Ting Yin;Ruirui Wang;Shaozhe Yang  NaN  Research Article;  1/24/2024 0:00   \n",
       "\n",
       "   ... original_paper_date          original_paper_doi  \\\n",
       "0  ...     10/27/2021 0:00  10.1007/s10876-021-02192-4   \n",
       "\n",
       "  original_paper_pubmed_id retraction_nature  \\\n",
       "0                      0.0        Retraction   \n",
       "\n",
       "                                              reason paywalled  \\\n",
       "0  +Concerns/Issues About Image;+Concerns/Issues ...        No   \n",
       "\n",
       "                                               notes  \\\n",
       "0  See also: https://pubpeer.com/publications/739...   \n",
       "\n",
       "                                         reason_list severity_score  \\\n",
       "0  ['Concerns/Issues About Image', 'Concerns/Issu...              4   \n",
       "\n",
       "                    subject  \n",
       "0  (BLS) Biology - Cellular  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load input .csv data into data frame  \n",
    "\n",
    "df = pd.read_csv(input_path, encoding='latin-1')\n",
    "\n",
    "# Visualize data frame\n",
    "\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f4eb0d-7229-45ea-8372-71996b1bbcbe",
   "metadata": {},
   "source": [
    "## Downloading JSON Files: Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682cba65-a5ca-4075-9da1-49de46fb4270",
   "metadata": {},
   "source": [
    "\n",
    "Our goal in this notebooks will be to download all the information that Open Alex has on the retracted papers that we are investigating. One can do that by making an API call to the OpenAlex database, using an URL that is specific to each paper. Luckily, this URL can easily be constructed from that paper's DOI. \n",
    "\n",
    "Since we will need to use it in what follows, let us go ahead and define a function that constructs an OpenAlex-appropriate URL give a papers DOI:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ecb9caae-d1d5-438e-b586-53fc11a0e28c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define address_builder function\n",
    "\n",
    "def address_builder(doi):\n",
    "    \"\"\"Takes a DOI identifier and builds the full URL address to perform an API call\n",
    "    on OpenAlex from it\"\"\"\n",
    "    \n",
    "    # Build url address and store it in string   \n",
    "    \n",
    "    base_address = \"https://api.openalex.org/works/https://doi.org/\" + doi\n",
    "    polite_address = base_address + \"?mailto=\" + \"pabloruizdeolano@gmail.com\" # Use polite address for faster API call performance\n",
    "    \n",
    "    # Return url address\n",
    "    \n",
    "    return polite_address\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad233879-5c8c-4fcd-ac16-15303b9c0383",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Next, we will define a function that systematically accesses whatever information OpenAlex contains for all our papers, and stores it in a .json file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "68ba3e4e-aa8b-43a8-94fd-b19024829d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define function to obtain .json files for all papers in our data frame\n",
    "\n",
    "\"\"\"\n",
    "Function takes a data frame two file paths to two directories, extracts a list of DOIs from\n",
    "the \"original_paper_doi\" column of input data frame and performs one API call per DOI, \n",
    "writes outcome as .json file in one of the two specified directories. If also keeps a log of successful and\n",
    "failed API calls, writes log as a .csv file in second directory.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def fetch_json_files(df, json_directory, log_directory):\n",
    "    \n",
    "    # Create empty list to store log with success or failure of each API call\n",
    "    \n",
    "    log = []\n",
    "    \n",
    "   # For loop to perform one API call per DOI in input data frame\n",
    "\n",
    "    for doi in df['original_paper_doi']:\n",
    "    \n",
    "        # Skip empty or invalid DOIs\n",
    "    \n",
    "        if not isinstance(doi, str) or not doi.strip(): \n",
    "            log.append({'DOI': doi, 'Status': 'Skipped - Empty or Invalid DOI'}) \n",
    "            continue  \n",
    "    \n",
    "        try:\n",
    "            # Build url address by calling address_builder function\n",
    "            \n",
    "            url = address_builder(doi)\n",
    "        \n",
    "            # Perform API call using URL address and store result in variable\n",
    "            \n",
    "            response = requests.get(url, timeout=10)  # Added timeout to prevent hanging requests\n",
    "        \n",
    "            # If clause to control for case in which API call fails\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "            \n",
    "                # Convert result of API call to json format\n",
    "            \n",
    "                data = response.json()\n",
    "            \n",
    "                # Create file path to save .json file with result of API call\n",
    "            \n",
    "                full_path = os.path.join(json_directory, doi.replace('/', '_') + '.json')\n",
    "            \n",
    "                # Save result of API call to .json file\n",
    "                \n",
    "                with open(full_path, 'w') as file:\n",
    "                    json.dump(data, file)\n",
    "            \n",
    "                # Update log list with dictionary specifying success for current DOI\n",
    "            \n",
    "                log.append({'DOI': doi, 'Status': 'Success'})\n",
    "        \n",
    "            else:\n",
    "                # Update log list with dictionary specifying failure for current DOI\n",
    "                \n",
    "                log.append({'DOI': doi, 'Status': f\"Failed - {response.status_code}\"})\n",
    "    \n",
    "        except requests.RequestException as e:\n",
    "            \n",
    "            # Handle exceptions during the API call (e.g., connection errors, timeouts)\n",
    "            \n",
    "            log.append({'DOI': doi, 'Status': f\"Failed - {str(e)}\"})\n",
    "\n",
    "    # Convert log list to data frame \n",
    "    \n",
    "    df_log = pd.DataFrame(log)\n",
    "    \n",
    "    # Write content of log data frame into resulting path\n",
    "    \n",
    "    df_log.to_csv(log_directory, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df8a16-81e1-4d63-93e8-84aa652c2510",
   "metadata": {},
   "source": [
    "## Downloading JSON Files: Test Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bca16a-a1d9-419c-b66e-4f6cfec58760",
   "metadata": {},
   "source": [
    "\n",
    "Having defined those functions, we can go ahead and start downloading information for our retracted papers from OpenAlex. Since data sets of interest will typically be quite large, we will first do that on a smaller sample, just to make sure that everything works properly. \n",
    "\n",
    "Let us first generate the required sample data frame, with some desired sample size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2fa6078c-6185-40c1-b180-d3e1b218dc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define sample size\n",
    "\n",
    "sample_size = 20\n",
    "\n",
    "# Check if sample_size is less than the number of rows in the data frame\n",
    "\n",
    "if sample_size <= len(df):\n",
    "    \n",
    "    # Create a random sample of the data frame with the defined sample size\n",
    "    \n",
    "    df_sample = df.sample(n=sample_size, random_state=1)  \n",
    "    \n",
    "else:\n",
    "    \n",
    "    print(\"Sample size is larger than the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e7af98-0103-4134-b31b-2347b635e9e4",
   "metadata": {},
   "source": [
    "\n",
    "We can now call our master function to store all the available information about these papers in .json files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "99b172d7-f2d4-462b-93cf-9f899bcdc163",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call fetch_and_log_data function to download data for sample data frame\n",
    "\n",
    "fetch_json_files(df_sample, json_directory, log_directory_api_outcome)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc849d4-e96a-47a2-ab67-3529eb801d42",
   "metadata": {
    "tags": []
   },
   "source": [
    "Because we will be retreieving information for many retracted papers, and because API calls are often slow, downloading all the information that we are interested in will take a considerable amount of time. It will therefore be convenient to devise a system with which we can keep track of what items have been already downloaded and resume the process from there in case any interrumptions take place. We will do devise one such system now, which we will use to see how our first test run at downloading information for our papers went.\n",
    "\n",
    "To do that, let us start by defining a function that checks what are the papers for which we have already been downloaded a .json file, then returns a data frame with the DOIs of those papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c617a406-74bf-41c0-9fa1-5a672561a32b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define function to get list of already downloaded DOIs\n",
    "\n",
    "def downloaded_paper_list_getter(directory, log_directory):\n",
    "    \"\"\"\n",
    "    Function takes a file path as input, checks how many .json files there are in that\n",
    "    direcotry, then reconstructs the DOIs associated to each file from their names by \n",
    "    removing the file extension, returns a data frame with resulting DOIs and writes\n",
    "    the content of this data frame into a .csv file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create list with names of all files in input directory\n",
    "    \n",
    "    file_names = [file for file in os.listdir(directory) if file.endswith('.json')]\n",
    "    \n",
    "    # Create list with names of all files minus \".json\" extension  \n",
    "    # Given the name structure of our files, this will give us a list of all DOIs in folder\n",
    "    \n",
    "    paper_dois = [file[:-5].replace('_', '/') for file in file_names]\n",
    "    \n",
    "    # Create data frame with names of all files in folder\n",
    "    \n",
    "    df_dois = pd.DataFrame(paper_dois, columns=['doi'])\n",
    "    \n",
    "    # Write content of log data frame into resulting path\n",
    "    \n",
    "    df_dois.to_csv(log_directory, index=False)\n",
    "    \n",
    "    # Return data frame\n",
    "    \n",
    "    return df_dois\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a5d07-8ca1-4c68-bdd3-b2b763d55f0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Let us call this function to generate a data frame with the DOIs of all the papers for which we already have a .json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ce5a66b1-13c5-463b-a77e-8ca49b26d2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8448, 1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Call function to generate data frame with DOIs of downloaded papers\n",
    "\n",
    "existing_doi_df = downloaded_paper_list_getter(json_directory, log_directory_doi_list)\n",
    "\n",
    "# Check size of resulting data frame\n",
    "\n",
    "existing_doi_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cf72e-5b9a-40c2-92a2-bfd14ae85c01",
   "metadata": {},
   "source": [
    "\n",
    "We can now inspect the information that the content of the .json files that were created for our sample data frame, and the log files that were generated in the process. If it all looks good, we can go ahead and download information for the rest of our retracted papers by using the functions that we defined earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a444fc1-31f4-44d0-abb5-e5b10fe08712",
   "metadata": {},
   "source": [
    "\n",
    "## Output: Downloading JSON Files for Entire Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620873a4-a355-4163-a8c5-f4bbd37877a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "To download .json files for our entire data set, it will be useful to define a new function that removes the papers for which we already have .json files available:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1f94d8c3-0856-4a0a-991e-6f5ad4861f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define function to remove papers for which we already have a .json file from original data frame\n",
    "\n",
    "def non_downloaded_papers_selector(df, existing_doi_df):\n",
    "    \"\"\"\n",
    "    Function takes an input data frame and a data frame with a list of DOIs, returns \n",
    "    the input data frame without those papers whose DOIs where included in the list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create data frame with DOIs of papers that have not been downloaded only\n",
    "    \n",
    "    df_not_downloaded = df[~df['original_paper_doi'].isin(existing_doi_df['doi'])]\n",
    "    \n",
    "    # Return data frame\n",
    "    \n",
    "    return df_not_downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdf802-396d-4724-a3ec-32a43f65f720",
   "metadata": {},
   "source": [
    "We can call this function to obtain a data frame which only contains papers for which a .json file still has to be downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aaaba851-352b-42b8-83e8-ae5e1e96d421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create data frame with DOIs of papers for which no data has been downloaded\n",
    "\n",
    "df_not_downloaded = non_downloaded_papers_selector(df, existing_doi_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20700588-07af-4ad3-b4b5-463dbbfcbc54",
   "metadata": {},
   "source": [
    "\n",
    "Having done that, we can call our master function to download .json files for all the remaining papers in our data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "274d3405-dd44-462c-9cce-c0aead5dfe7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fetch_json_files(df_not_downloaded, json_directory, log_directory_api_outcome)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81f46b-cd5e-430e-a3fb-f6d473780a0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note that, should the process be interrupted, we can always repeat this process to restart it right were it was left. This is, in fact, the system to cope with possible interruptions that we mentioned earlier.\n",
    "\n",
    "To resume the process, we can simply proceed as we did above. First we obtain a data frame with the DOIs of those papers for which a .json file was downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "464e7033-a91d-4277-b808-85e5ff1fb649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9102, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Call function to obtain data frame with DOIs of papers for which we have a .json file\n",
    "\n",
    "existing_doi_df = downloaded_paper_list_getter(json_directory, log_directory_doi_list)\n",
    "\n",
    "# Check number of papers for which a .json file was downloaded\n",
    "\n",
    "existing_doi_df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44dacae-d9a1-4176-928e-6889e4a4da67",
   "metadata": {
    "tags": []
   },
   "source": [
    "Then we remove those papers for which data was already downloaded from our data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2cd08812-92cc-41c3-9fd9-ffb8419f1dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(675, 22)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Call function to remove downloaded papers from data frame\n",
    "\n",
    "df_not_downloaded = non_downloaded_papers_selector(df, existing_doi_df)\n",
    "\n",
    "# Check number of papers for which a .json file has not been downloaded\n",
    "\n",
    "df_not_downloaded.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15bd18e-5811-4c7e-98da-e6e926aa9df9",
   "metadata": {},
   "source": [
    "And finally we call our master functio to resume the process once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "63a86d7d-4424-4ec4-9406-33a9d09a1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Call master function to downloaded remaining .json files\n",
    "\n",
    "fetch_json_files(df_not_downloaded, json_directory, log_directory_api_outcome)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
