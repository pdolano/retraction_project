{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c0d779-9f5c-4cc8-9229-2154c7bf0fe6",
   "metadata": {},
   "source": [
    "# 2c. Extract Abstracts for Retracted Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ad1d4-6973-4144-b34e-84fa2296c3ff",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f358f2-9bf8-4585-91d6-9b75be1b6a4d",
   "metadata": {},
   "source": [
    "This Notebook **extracts abstracts** from the JSON files that we downloaded from OpenAlex **for our retracted papers**. It does so by extracting inverted word indeces first, and then reconstructing them to obtain readable abstract text.\n",
    "\n",
    "The Notebook thus takes as input the JSON files that were downloaded in **Notebook 2a**. The reconstructed abstract generated in here will in turn be used as input in **Notebook 5** in order to generate the corpus of text from both retracted and non-retracted papers that we will used to train our model (Notebooks in the \"3\" series will extract and pre-process text for non-retracted papers, hence the numbering convention).\n",
    "\n",
    "The **workflow** of the Notebook has been set to be as follows:\n",
    "\n",
    "- Input: as many **JSON files** as retracted papers there are under investigation.\n",
    "- Output: **one .csv file** with the reconstructed abstracts, along with the DOI, year, and country of each paper, and **one .csv file** with log information about the extraction and reconstruction process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e243347-13bd-44ca-ab3a-a41de4e80465",
   "metadata": {},
   "source": [
    "## Input / Output Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc1d96-0d92-4e02-9696-d9205a7613f7",
   "metadata": {},
   "source": [
    "Input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9923da9-d01b-4f5f-9398-28f319d26ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Input path\n",
    "\n",
    "input_path = '../data/json_files/cell_biology/retracted'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb7273-9c36-4fcc-8939-3bb7a38f98e0",
   "metadata": {},
   "source": [
    "Output parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb4aecbd-7433-4b87-9169-44f25727858a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Output path for abstracts\n",
    "\n",
    "output_path_abstracts = '../data/abstracts/cell_biology/retracted/retracted_cell_biology_abstracts.csv'\n",
    "\n",
    "# Output path for logs\n",
    "\n",
    "output_path_logs = '../data/logs/retracted_abstract_extraction_log.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae060ff-f297-411b-bb86-0b41f59261c3",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a88944e2-31ad-497d-9b7a-507db16ed5cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c05fc24-0b42-440e-8bf9-3ff5914cd2b4",
   "metadata": {},
   "source": [
    "## Extracting Abstracts from JSON Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f3280-75f7-4d85-8b11-a1550ab75087",
   "metadata": {},
   "source": [
    "We will use one master function to extract and store the abstracts from the .json files that we downloaded from OpenAlex for non-retracted papers. Let us go ahead and define the function in question, which we will use again in a future notebook to extract abstracts from non-retracted papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "564a3f80-1548-477c-8861-fefbca9dd592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define abstract_getter function\n",
    "\n",
    "def abstract_getter(input_path, target_val):\n",
    "    \n",
    "    # Initialize variables\n",
    "    data = []\n",
    "    log_entries = []\n",
    "    \n",
    "    # Obtain file names of .json files in input directory\n",
    "    files = os.listdir(input_path)\n",
    "    \n",
    "    # For loop to iterate over all .json files in input directory\n",
    "    for i, filename in enumerate(files):\n",
    "    \n",
    "        # If sentence to make sure we only process .json files\n",
    "        if filename.endswith('.json'):\n",
    "        \n",
    "            # Construct full path for current .json file in loop\n",
    "            file_path = os.path.join(input_path, filename)\n",
    "        \n",
    "            # Try sentence to account for errors reading current .json file\n",
    "            try:\n",
    "            \n",
    "                # Open and read current .json file\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                \n",
    "                    # Write content of current .json file into content variable\n",
    "                    content = json.load(file)\n",
    "                \n",
    "                    # Extract DOI of article associated to current .json file\n",
    "                    doi = filename.replace('.json', '')\n",
    "\n",
    "                    # Get inverted index from content variable \n",
    "                    abstract_inverted_index = content.get('abstract_inverted_index', {})\n",
    "                \n",
    "                    # If sentence to make sure we only process non-empty word indeces\n",
    "                    if abstract_inverted_index:\n",
    "                    \n",
    "                        # Initialize list of tuples with each word and the position that it occupies in the abstract text\n",
    "                        index_word_pairs = [(index, word) for word, indices in abstract_inverted_index.items() for index in indices]\n",
    "                    \n",
    "                        # Sort list according to the position of each word in the abstract text\n",
    "                        index_word_pairs.sort()\n",
    "                    \n",
    "                        # Reconstruct abstract by adding words in list of tuples in the order of their occurrence in the text\n",
    "                        # We add a space at the beginning and strip it at the end\n",
    "                        abstract_text = ' '.join(word for _, word in index_word_pairs).strip()\n",
    "                   \n",
    "                        # Create string with delimiter characters to be removed from reconstructed text\n",
    "                        delimiters = \",;|{}\\n\\r\\t[]<>\"\n",
    "                    \n",
    "                        # For loop to iterate over all delimiters in delimiter string\n",
    "                        for delimiter in delimiters:\n",
    "                        \n",
    "                            # Replace current delimiter in loop with blank space\n",
    "                            abstract_text = abstract_text.replace(delimiter, ' ')\n",
    "                        \n",
    "                        # Call function to remove non printable characters from reconstructed text \n",
    "                        abstract_text = remove_non_printable(abstract_text)\n",
    "                    \n",
    "                    # Else sentence to account for situation in which inverted index is empty\n",
    "                    else:\n",
    "                        log_entries.append({'filename': filename, 'success': False, 'message': 'No abstract_inverted_index provided'})\n",
    "                        continue\n",
    "                    \n",
    "                    # Initialize author_country string\n",
    "                    author_country = 'Unknown'  # Default value\n",
    "                \n",
    "                    # If clause to check if authorship information is present in content variable\n",
    "                    if 'authorships' in content:\n",
    "                    \n",
    "                        # For loop to iterate over authorsips list\n",
    "                        for authorship in content['authorships']:\n",
    "                        \n",
    "                            # Extract country code from institution of first author for which information is available\n",
    "                            if 'institutions' in authorship and any(inst.get('country_code') for inst in authorship['institutions']):\n",
    "                                author_country = next((inst['country_code'] for inst in authorship['institutions'] if 'country_code' in inst), \"Unknown\")\n",
    "                                break\n",
    "\n",
    "                    # Extract publication year from content variable\n",
    "                    year = content.get('publication_year', 'Unknown')\n",
    "                \n",
    "                    # Check for the presence of \"retract%\" or \"withdraw%\" in abstract_text\n",
    "                    retracted_flag = any(word in abstract_text.lower() for word in [\"retract\", \"withdraw\", \"retracted\", \"retraction\", \"withdrew\", \"withdrawal\",\"withdrawn\", \"retracts\"])\n",
    "\n",
    "                    # Update data variable with reconstructed text and additional information\n",
    "                    data.append({\n",
    "                        'abstract_text': f'\"{abstract_text}\"',  # Ensure the text is surrounded by double quotes\n",
    "                        'target': target_val,\n",
    "                        'doi': doi,\n",
    "                        'country': author_country,\n",
    "                        'year': year,\n",
    "                        'ret_flag': retracted_flag\n",
    "                    })\n",
    "\n",
    "                    # Update log variable  with success message\n",
    "                    log_entries.append({'filename': filename, 'success': True, 'message': 'Processed successfully'})\n",
    "        \n",
    "        \n",
    "            # Clause to account for errors in reading the current .json file\n",
    "            except Exception as e:\n",
    "            \n",
    "                # Update log variable with current error message\n",
    "                log_entries.append({'filename': filename, 'success': False, 'message': f'Error processing file - {str(e)}'})\n",
    "\n",
    "    return data, log_entries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e104870-cc2d-4e30-a015-cbfdcc61068c",
   "metadata": {},
   "source": [
    "The function uses a smaller function to remove non-printable characters from our abstract text, which we go on to define as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1baff95-3982-4857-aa6a-f06b7df9e47e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define function to remove non-printable characters\n",
    "\n",
    "def remove_non_printable(text):\n",
    "    printable = set(string.printable)\n",
    "    return ''.join(filter(lambda x: x in printable, text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49164f5-1d6e-42bb-850d-55c8c3f4aff4",
   "metadata": {},
   "source": [
    "We can now go on to call our master function to extract the abstract information that we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d60645a-0516-4aa6-8b1a-c270420eefd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store data and log entries\n",
    "\n",
    "data = []\n",
    "log_entries = []\n",
    "\n",
    "data, log_entries = abstract_getter(input_path, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b3df5-028f-4ca4-8aaa-a50764544e42",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3deb6e4-7b14-486c-ab58-d5566e9e0196",
   "metadata": {},
   "source": [
    "To conclude, we save the abstract text that we extracted from our .json files into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af43ef2d-222c-4054-bc3e-5b71c8f8760e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create data frame with content of data list and pipe symbol as delimiter\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save content of data frame to .csv\n",
    "\n",
    "df.to_csv(output_path_abstracts, sep='|', index=False, quoting=csv.QUOTE_MINIMAL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b9c20f-7ca0-4c8e-a48d-ec531f19fb59",
   "metadata": {},
   "source": [
    "And we do the same with the log files that we generated in the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1063b19e-0a39-4623-a0e3-34897e74f4ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create data frame with content of log_entries list \n",
    "\n",
    "log_df = pd.DataFrame(log_entries)\n",
    "\n",
    "# Save content of log entry data frame to .csv\n",
    "\n",
    "log_df.to_csv(output_path_logs, index=False, quoting=csv.QUOTE_MINIMAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7741a0c-ab71-4c99-9ccb-0879eedfca58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
