{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5dd072-bb10-4932-8598-5d3aecba0e48",
   "metadata": {},
   "source": [
    "# 3a. Fetching DOIs of Non-Retracted Papers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cd73e-5aff-4763-b120-1882a4896458",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af693019-1299-4c1c-a455-28cca2f91a78",
   "metadata": {},
   "source": [
    "This Notebook **obtains the DOIs of a number of randomly selected non-retracted papers, whose year and country specifications match those of the retracted papers under investigation**. In order to do that, we will pick the DOIs for our non-retracted papers from a number of \"buckets\" that we will define for each year and country. These buckest will be built so as to contain as many non-retracted papers as retracted papers we had in our original data base, for each year and country. \n",
    "\n",
    "The Notebook takes as input the .csv file that was generated by **script 3b**, which contained the size of the bucket associated to each year and country. \n",
    "\n",
    "The **workflow** of the notebook is therefore as follows:\n",
    "\n",
    "- Input: **two .csv files**. The first one contains the bucketing specifications obtained from script 3b, whereas the second one gives us the appropriate code for each country, which we will use to add the appropriate filters to our search.\n",
    "\n",
    "- Output: **one .csv file** with the DOIs of non-retracted papers gathered from our buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db77eb-9d5d-4ebc-8b74-f27acd4eb7dd",
   "metadata": {},
   "source": [
    "## Input / Output Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c809d-2f22-4af8-a683-9a9685d44889",
   "metadata": {},
   "source": [
    "Input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db710a39-004d-4279-868e-d3469f5048cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# File path for .csv input file with bucketing specifications\n",
    "\n",
    "input_path = \"../data/Country_Year_Buckets_Cellbio.csv\"\n",
    "\n",
    "# Id of subfield to fitler our paper search \n",
    "# Id value for cell_bio is 1307\n",
    "\n",
    "subfield_filter_value = \"1307\"\n",
    "\n",
    "# File path for ISO country code equivalences\n",
    "\n",
    "input_path_dictionary = \"../data/country_code_dictionary.csv\"\n",
    "\n",
    "# Upsize factor for each bucket\n",
    "\n",
    "upsize_factor = 1.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effe7e7-ce8f-4521-9936-ab38f8e5cc1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Output parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05e7dbf-cc3e-4f83-8cb6-4081153c4102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# File name for .csv with DOIs of non-retracted papers\n",
    "\n",
    "output_file_name = \"dois_jenny_corrected_3.csv\"\n",
    "\n",
    "# File name for .csv with unique DOIs of non-retracted papers\n",
    "\n",
    "output_path_unique = \"../data/results/dois_jenny_unique.csv\"\n",
    "\n",
    "# File path for .jsonl file with text data for abstracts\n",
    "\n",
    "output_path = \"../data/results/\" + output_file_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45085f7-21d4-460c-8d59-fd8a28ac73fb",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989ee18-aab1-4569-b140-f84b86beed73",
   "metadata": {},
   "source": [
    "\n",
    "As always, let's start by importing all required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4fa866-c999-433d-bb8f-0eabffc89178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from json.decoder import JSONDecodeError\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68239fe-4b98-4391-a0f5-23f8347e3baa",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed31b87-d76c-46ae-9bd6-78105f9234ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "And by loading the relevant data from our .csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b908b82-7ce1-448f-a292-a2a4b3521e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load .csv with bucketing specifications into data frame\n",
    "\n",
    "df = pd.read_csv(input_path, encoding='latin-1', sep = \";\")\n",
    "\n",
    "# Load .csv file with ISO country code equivalences\n",
    "\n",
    "df_country_codes = pd.read_csv(\"../data/country_codes_dictionary.csv\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215177a-6c7b-4101-a83c-f96e9c69c4df",
   "metadata": {},
   "source": [
    "-The data specifying our contry code equivalences requires some cleaninig, so we will go ahead and make the necessary adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10835b3-a832-4792-a8df-b0d1987d76db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Clean spurious spaces in \"Country\" column\n",
    "\n",
    "df_country_codes['Country'] = df_country_codes['Country'].str.strip()\n",
    "\n",
    "# Clean spurious spaces in \"TIS\" column\n",
    "\n",
    "df_country_codes['TIS'] = df_country_codes['TIS'].str.strip()\n",
    "\n",
    "# Create country code dictionary from countr code data frame\n",
    "\n",
    "country_codes_dictionary = df_country_codes.set_index('Country')['TIS'].str.strip().to_dict()\n",
    "\n",
    "# Rename country column \n",
    "\n",
    "df.rename(columns={\"ï»¿country\": \"country\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3916d0-ffea-4a49-91a7-de6a6364ce8a",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5359f6a-e317-40d8-bdec-040a93684803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seconds_to_hms(seconds):\n",
    "    \"\"\"\n",
    "    Convert seconds to hours, minutes, and seconds.\n",
    "\n",
    "    Parameters:\n",
    "    seconds (int): Number of seconds.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the equivalent time in hours, minutes, and seconds.\n",
    "    \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "\n",
    "    return hours, minutes, seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f42aeb-139c-4a79-842f-7b34dd22ef82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def url_builder(country, year, field_id, page = \"1\"):\n",
    "    \"\"\"This function builds the URL that OpenAlex requires to find papers with the characteristics\n",
    "    specified in the filters below.\"\"\"\n",
    "    \n",
    "    # Add subfield filter to our URL\n",
    "    \n",
    "    subfield_filter = \"primary_topic.subfield.id:\" + field_id\n",
    "    \n",
    "    # Call country_code_getter to obtain code for year\n",
    "    \n",
    "    country_code = country\n",
    "        \n",
    "    # Add publication year filter to our URL\n",
    "    \n",
    "    year_filter = \"publication_year:\" + year\n",
    "\n",
    "    # Add retraction filter\n",
    "\n",
    "    retraction_filter = \"is_retracted:false\"\n",
    "    \n",
    "    # Add country filter to our URL\n",
    "    \n",
    "    country_filter = \"institutions.country_code:\" + country_code\n",
    "    \n",
    "    # Add type of work giler\n",
    "    \n",
    "    type_filter = \"type:article\"\n",
    "    \n",
    "    # Add page number\n",
    "    \n",
    "    page_number = page\n",
    "    \n",
    "    # Add filters to base url\n",
    "    \n",
    "    url = \"https://api.openalex.org/works?page=\" + page_number + \"&per-page=200&filter=\" + subfield_filter + \",\" + year_filter + \",\" + country_filter + \",\" + type_filter + \",\" + retraction_filter\n",
    "\n",
    "    # Return full URL\n",
    "    \n",
    "    return url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b81958a-d689-4daa-9508-cb515986ea35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def doi_getter_pagination(country, year, field, paper_num):\n",
    "    \n",
    "    \"\"\"Function takes the URL to perform an API search in OpenAlex with a number of \n",
    "    search filters. It then extracts the DOIs of the specified number of papers, then \n",
    "    returns a list with the DOIs in question\"\"\"\n",
    "\n",
    "    doi_lst = []  # Initialize list to store DOIs\n",
    "    page = 1  # Initialize page number\n",
    "    \n",
    "    # Calculate required page number\n",
    "    \n",
    "    page_num = int(paper_num / 200) + 1\n",
    "    \n",
    "    # Perform API calls until the desired number of DOIs is collected\n",
    "    \n",
    "    for page in range(1, page_num + 1):\n",
    "\n",
    "        # Update search URL with page number\n",
    "        \n",
    "        page_code = str(page)\n",
    "        url = url_builder(country,year,field,page_code)\n",
    "        \n",
    "        # Perform API call and decode JSON result to obtain meta-data\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        meta_data = response.json()\n",
    "        \n",
    "        # Extract DOIs from API response and add to doi_lst\n",
    "        \n",
    "        # Extract DOIs from API response and add to doi_lst\n",
    "        \n",
    "        for element in meta_data[\"results\"]:\n",
    "            if element[\"doi\"] is not None:\n",
    "                doi_lst.append(element[\"doi\"])\n",
    "            if len(doi_lst) >= paper_num:\n",
    "                return list(set(doi_lst))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6048f298-2050-4144-8173-2c2b179c1088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def doi_from_address(address_lst):\n",
    "    \"\"\"Function takes the whole URL address associated to the DOI of a given paper, \n",
    "    then subtracts the first part of the address to obtain its DOI code only. It\n",
    "    takes as input a list of DOI URLs, returns a list of DOI codes\"\"\"\n",
    "\n",
    "    doi_lst = []\n",
    "    \n",
    "    for element in address_lst:\n",
    "        doi_lst.append(element.removeprefix(\"https://doi.org/\"))\n",
    "        \n",
    "    return doi_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "989b0eea-ae85-4c20-9fce-52082447d21f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def address_builder(doi):\n",
    "    \"\"\"Takes a DOI identifier and builds full URL address from it, with format\n",
    "    required for a normal OpenAlex API call\"\"\"\n",
    "    \n",
    "    # Store url addresses in string\n",
    "    \n",
    "    base_address = \"https://api.openalex.org/works/https://doi.org/\" + doi\n",
    "    polite_address = base_address + \"?mailto=\" + \"pabloruizdeolano@gmail.com\" # Use polite address for faster API call performance\n",
    "    \n",
    "    # Return polite address\n",
    "    \n",
    "    return polite_address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c88a570-4b3a-40a5-8aa6-4a85833b180f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def meta_data_extractor(doi, n_grams=\"False\"):\n",
    "    \"\"\"Function takes a DOI and calls address_builder function to build full URL. \n",
    "    It then performs an API call, and it returns the metadata as JSON dictionary.\n",
    "    If n_grams is set to \"True\" it grabs the n_grams for the paper, if those \n",
    "    are avaiable. Otherwise it grabs its abstract word index.\"\"\"\n",
    "    \n",
    "    # Build correct address depending on value of \"n_grams\" parameter\n",
    "    \n",
    "    if n_grams == \"False\":\n",
    "        url = address_builder(doi)\n",
    "    else:\n",
    "        url = address_builder_ngrams(doi)\n",
    "    \n",
    "    # Perform API call and store result in response variable\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Convert meta-data to json format if possible and store result in variable\n",
    "    \n",
    "    try:\n",
    "        meta_data = response.json()\n",
    "    except JSONDecodeError: # This is in case result of API call was not in JSON format\n",
    "        print(\"Error: Unable to decode JSON response\")\n",
    "        meta_data = None\n",
    "    \n",
    "    return meta_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "534ea179-7bb3-4a5e-8075-680c93e70df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def pick_random_entries(string, n):\n",
    "    \n",
    "    # Generate n random indices within the range of the string length\n",
    "    \n",
    "    random_indices = random.sample(range(len(string)), n)\n",
    "    \n",
    "    # Select the characters at the random indices\n",
    "    \n",
    "    random_entries = [string[i] for i in random_indices]\n",
    "    \n",
    "    return random_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c274b-0d80-41a4-9eb4-d23a1036b9e6",
   "metadata": {},
   "source": [
    "## Getting DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948d12c1-c713-4632-bcc6-22f59192f436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m triple_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mcount\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Call doi_getter to get list as many filtered DOI URLs as our sample size\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m doi_list \u001b[38;5;241m=\u001b[39m \u001b[43mdoi_getter_pagination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1307\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriple_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Write log entry with number of DOIs grabbed for this iteration of the loop\u001b[39;00m\n\u001b[1;32m     35\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+++++++++++++++++++++++++++++++++++++++++++\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mdoi_getter_pagination\u001b[0;34m(country, year, field, paper_num)\u001b[0m\n\u001b[1;32m     21\u001b[0m url \u001b[38;5;241m=\u001b[39m url_builder(country,year,field,page_code)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Perform API call and decode JSON result to obtain meta-data\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m meta_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Extract DOIs from API response and add to doi_lst\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Extract DOIs from API response and add to doi_lst\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Open log file\n",
    "\n",
    "with open(\"../data/logs/\" + output_file_name + \"logfile.txt\", \"w\") as f:\n",
    "\n",
    "    f.write(\"Log file opened.\\n\")\n",
    "    \n",
    "    # Store start time of loop execution\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize list to store dois\n",
    "\n",
    "    final_doi_list = []\n",
    "\n",
    "    # For loop to gather dois\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        # Intialize variables\n",
    "    \n",
    "        year = str(row['year'])\n",
    "    \n",
    "        country = str(row['country'])\n",
    "    \n",
    "        count = row['count']\n",
    "    \n",
    "        triple_count = 3*count\n",
    "\n",
    "        # Call doi_getter to get list as many filtered DOI URLs as our sample size\n",
    "            \n",
    "        doi_list = doi_getter_pagination(country, year, \"1307\", triple_count)\n",
    "    \n",
    "        # Write log entry with number of DOIs grabbed for this iteration of the loop\n",
    "    \n",
    "        f.write(\"+++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        f.write(f\"LOOP NUMBER {index + 1}: Year={year}, COUNTRY={country}, COUNT={count}. \\n\")\n",
    "        f.write(f\"Picked {len(doi_list)} DOIs, triple count was {triple_count}. \\n\")\n",
    "        f.write(f\"Of those, {len(set(doi_list))} were unique DOIs \\n\\n\")\n",
    "\n",
    "        # Obtain size of upsized sample from upsize factor defined at the beginning\n",
    "    \n",
    "        upsized_sample = int(count * upsize_factor)\n",
    "    \n",
    "        # Randomly get a number of dois equal to a slightly upsized sample size\n",
    "    \n",
    "        if len(doi_list) > 0 and len(doi_list) > upsized_sample:\n",
    "            doi_list = pick_random_entries(doi_list, upsized_sample)\n",
    "        else:\n",
    "            doi_list = []\n",
    "        \n",
    "        # Writing log entry with number of DOIs randomly grabbed at this point\n",
    "    \n",
    "        f.write(f\"I then picked {len(doi_list)} DOIs randomly. \\n\")\n",
    "        f.write(f\"Of these, {len(set(doi_list))} were unique DOIs. \\n\")\n",
    "        f.write(f\"Target count was {count}.\\n\")\n",
    "        \n",
    "        if len(set(doi_list)) != len(set(doi_list)):\n",
    "            f.write(f\"ERROR: WE HAVE {len(doi_list)} - {len(set(doi_list))} REPEATED DOIS. \\n\")\n",
    "        \n",
    "        # Add result of current iteration to final list of dois\n",
    "    \n",
    "        final_doi_list.extend(doi_list) \n",
    "\n",
    "    # Calculate elapsed time for for lopp execution\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_hours, elapsed_minutes, elapsed_seconds = seconds_to_hms(elapsed_time)\n",
    "\n",
    "    print(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {round(elapsed_seconds,1)}s. \\n\")\n",
    "\n",
    "    # Write final message in log file\n",
    "    \n",
    "    f.write(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {elapsed_seconds}s. \\n\")\n",
    "    f.write(\"End of log file.\\n\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d339da5-1ae5-4f3c-9268-b05ebcc8e439",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe49e7f-c9bb-4451-98eb-f4d6f4fbd7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert list to data frame\n",
    "\n",
    "df_dois = pd.DataFrame(final_doi_list)\n",
    "\n",
    "# Write data frame to .csv\n",
    "\n",
    "df_dois.to_csv(output_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b7ed6-e37e-4374-9495-c582366a64e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create set with DOIs to get rid of repeated entries\n",
    "\n",
    "final_doi_set = set(final_doi_list)\n",
    "\n",
    "# Convert set to data frame\n",
    "\n",
    "df_unique_dois = pd.DataFrame(final_doi_set)\n",
    "\n",
    "# Write data frame to .csv\n",
    "\n",
    "df_unique_dois.to_csv(output_path_unique, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1e296-4c33-43c8-b04b-b2eb8b4ebb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python \"04c_extract_nonretract_abstract_as_text.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9dd95-6a11-45cc-a290-9c42cf946ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
