{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5dd072-bb10-4932-8598-5d3aecba0e48",
   "metadata": {},
   "source": [
    "# 3a. Fetching DOIs of Non-Retracted Papers \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cd73e-5aff-4763-b120-1882a4896458",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af693019-1299-4c1c-a455-28cca2f91a78",
   "metadata": {},
   "source": [
    "This Notebook **obtains DOIs of non-retracted papers**. The papers with these DOIs, furthermore, will be **randomly** selected from OpenAlex, and they will have the **same country and year distribution** as the retracted papers in our original data set. Needless to say, they will also be papers from the same discipline.\n",
    "\n",
    "The Notebook takes as input the .csv file that was generated by **Notebook 2c**, contained the country and year distribution of our retracted papers. \n",
    "\n",
    "The **workflow** of the notebook is therefore as follows:\n",
    "\n",
    "- Input: **one .csv file** with the country and year distribution of our retracted papers.\n",
    "- Output: **one .csv file** with the DOIs of non-retracted papers that we got from OpenAlex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db77eb-9d5d-4ebc-8b74-f27acd4eb7dd",
   "metadata": {},
   "source": [
    "## Input / Output Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c809d-2f22-4af8-a683-9a9685d44889",
   "metadata": {},
   "source": [
    "Input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db710a39-004d-4279-868e-d3469f5048cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# File path for .csv input file with bucketing specifications\n",
    "input_path = \"../data/Country_Year_Buckets_Cellbio.csv\"\n",
    "\n",
    "# Id of subfield to fitler our paper search \n",
    "# Id value for cell_bio is 1307\n",
    "subfield_filter_value = \"1307\"\n",
    "\n",
    "# File path for ISO country code equivalences\n",
    "input_path_dictionary = \"../data/country_code_dictionary.csv\"\n",
    "\n",
    "# Upsize factor for each bucket\n",
    "upsize_factor = 1.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effe7e7-ce8f-4521-9936-ab38f8e5cc1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Output parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05e7dbf-cc3e-4f83-8cb6-4081153c4102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Paths for previous draft\n",
    "\n",
    "# File name for .csv with DOIs of non-retracted papers\n",
    "#output_file_name = \"dois_jenny_corrected_3.csv\"\n",
    "\n",
    "# File path for .csv with unique DOIs of non-retracted papers\n",
    "#output_path_unique = \"../data/results/dois_jenny_unique.csv\"\n",
    "\n",
    "# File path for .jsonl file with text data for abstracts\n",
    "#output_path = \"../data/results/\" + output_file_name\n",
    "\n",
    "# Current path\n",
    "\n",
    "# File path for .csv with DOIs of non-retracted papers\n",
    "\n",
    "output_path = \"../data/dois_non_retracted/non_retracted_dois_cell_bio.csv\" \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45085f7-21d4-460c-8d59-fd8a28ac73fb",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989ee18-aab1-4569-b140-f84b86beed73",
   "metadata": {},
   "source": [
    "\n",
    "As always, let's start by importing all required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4fa866-c999-433d-bb8f-0eabffc89178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from json.decoder import JSONDecodeError\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68239fe-4b98-4391-a0f5-23f8347e3baa",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed31b87-d76c-46ae-9bd6-78105f9234ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "And by loading the relevant data from our .csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b908b82-7ce1-448f-a292-a2a4b3521e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load .csv with bucketing specifications into data frame\n",
    "df = pd.read_csv(input_path, encoding='latin-1', sep = \";\")\n",
    "\n",
    "# Load .csv file with ISO country code equivalences\n",
    "df_country_codes = pd.read_csv(\"../data/country_codes_dictionary.csv\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215177a-6c7b-4101-a83c-f96e9c69c4df",
   "metadata": {},
   "source": [
    "The data specifying our contry code equivalences requires some cleaninig, so we will go ahead and make the necessary adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10835b3-a832-4792-a8df-b0d1987d76db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Clean spurious spaces in \"Country\" column\n",
    "df_country_codes['Country'] = df_country_codes['Country'].str.strip()\n",
    "\n",
    "# Clean spurious spaces in \"TIS\" column\n",
    "df_country_codes['TIS'] = df_country_codes['TIS'].str.strip()\n",
    "\n",
    "# Create country code dictionary from countr code data frame\n",
    "country_codes_dictionary = df_country_codes.set_index('Country')['TIS'].str.strip().to_dict()\n",
    "\n",
    "# Rename country column \n",
    "df.rename(columns={\"ï»¿country\": \"country\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3916d0-ffea-4a49-91a7-de6a6364ce8a",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21699f0e-b65c-4f39-995e-05af825570b0",
   "metadata": {},
   "source": [
    "In order to extract DOIs of non-retracted papers from the same field, country, and year of publication as thsoe of our retracted papers, we will need to query OpenAlex to find articles with the right characteristics. OpenAlex uses URL addresses with certain characteristics for this purpose, which have the appropriate filters built into them. \n",
    "\n",
    "Our first task will therefore consist of creating a function that builds an an URL that we can use to make an API request to query OpenAlex, given our our desired filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f42aeb-139c-4a79-842f-7b34dd22ef82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define url_builder function\n",
    "\n",
    "def url_builder(country, year, field_id, page = \"1\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Builds the URL that OpenAlex requires to access bibliographic information via API\n",
    "    \n",
    "    Parameters: \n",
    "        country (str): country of papers to be queried\n",
    "        year (str): year of papers to be queried\n",
    "        field_id (str): id code of field of papers\n",
    "        page (str): number of page from which papers will be queried\n",
    "    \n",
    "    Returns:\n",
    "        url (str): full url to be used for OpenAlex API request\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Add subfield filter to our URL\n",
    "    subfield_filter = \"primary_topic.subfield.id:\" + field_id    \n",
    "        \n",
    "    # Add publication year filter to our URL\n",
    "    year_filter = \"publication_year:\" + year\n",
    "\n",
    "    # Add retraction filter to make sure queried papers are NOT retracted\n",
    "    retraction_filter = \"is_retracted:false\"\n",
    "    \n",
    "    # Add country filter to our URL\n",
    "    country_code = country\n",
    "    country_filter = \"institutions.country_code:\" + country_code\n",
    "    \n",
    "    # Add type of work filter\n",
    "    type_filter = \"type:article\"\n",
    "    \n",
    "    # Add page number\n",
    "    page_number = page\n",
    "    \n",
    "    # Add filters to base url\n",
    "    url = \"https://api.openalex.org/works?page=\" + page_number + \"&per-page=200&filter=\" + subfield_filter + \",\" + year_filter + \",\" + country_filter + \",\" + type_filter + \",\" + retraction_filter\n",
    "\n",
    "    # Return full URL\n",
    "    return url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1cc2a4-6711-43f8-8511-7a4cce8b7679",
   "metadata": {},
   "source": [
    "Next, let us define the function that we will use to access bibliographic information of non-retracted articles with a given set of specified characteristics. The function will our function \"url_builder,\" which we defined above, to make a query to OpenAlex, then extract information for as many papers as needed with from the required field, country, and year, then store it in a JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b81958a-d689-4daa-9508-cb515986ea35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Master function\n",
    "# Define doi_getter function\n",
    "\n",
    "def doi_getter(country, year, field, paper_num):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts DOIs of papers based on specified criteria.\n",
    "\n",
    "    Parameters:\n",
    "        country (str): The country associated with the papers.\n",
    "        year (str): The year of publication of the papers.\n",
    "        field (str): The field or discipline of the papers.\n",
    "        paper_num (int): The number of DOIs to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list containing the extracted DOIs of the papers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize list to store DOIs\n",
    "    doi_lst = []  \n",
    "    \n",
    "    # Calculate required page number\n",
    "    page_num = int(paper_num / 200) + 1\n",
    "    \n",
    "    # Perform API calls until the desired number of DOIs is collected\n",
    "    for page in range(1, page_num + 1):\n",
    "\n",
    "        # Update search URL with page number\n",
    "        page_code = str(page)\n",
    "        url = url_builder(country,year,field,page_code)\n",
    "        \n",
    "        # Perform API call and decode JSON result to obtain meta-data\n",
    "        response = requests.get(url)\n",
    "        meta_data = response.json()\n",
    "                \n",
    "        # Extract DOIs from API response and add to doi_lst\n",
    "        for element in meta_data[\"results\"]:\n",
    "            if element[\"doi\"] is not None:\n",
    "                doi_lst.append(element[\"doi\"])\n",
    "            if len(doi_lst) >= paper_num:\n",
    "                return list(set(doi_lst))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471255ae-7009-40c7-8b7e-03dd8e7808b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Our strategy will consist of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "534ea179-7bb3-4a5e-8075-680c93e70df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define pick_random_entries function\n",
    "\n",
    "def pick_random_entries(string, n):\n",
    "    \n",
    "    # Generate n random indices within the range of the string length\n",
    "    random_indices = random.sample(range(len(string)), n)\n",
    "    \n",
    "    # Select the characters at the random indices\n",
    "    random_entries = [string[i] for i in random_indices]\n",
    "    \n",
    "    return random_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a2f25-ae55-41ff-acfa-7c8429e87041",
   "metadata": {},
   "source": [
    "\n",
    "Since we will be downloading information for a considerable number of papers, it will sometimes be useful to have a sense of how each downloading session took. In order to be able to present that information neatly, we will define the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5359f6a-e317-40d8-bdec-040a93684803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define seconds_to_hms function\n",
    "\n",
    "def seconds_to_hms(seconds):\n",
    "    \"\"\"\n",
    "    Convert seconds to hours, minutes, and seconds.\n",
    "\n",
    "    Parameters:\n",
    "    seconds (int): Number of seconds.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the equivalent time in hours, minutes, and seconds.\n",
    "    \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "\n",
    "    return hours, minutes, seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c274b-0d80-41a4-9eb4-d23a1036b9e6",
   "metadata": {},
   "source": [
    "## Output: Getting DOIs for Non-Retracted Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975fc6f-dc1d-4863-b45a-f8f370928abf",
   "metadata": {},
   "source": [
    "We can now make use of these functions to obtain the desired number of DOIs for non-retracted papers for each year and country. We will write a log documenting how the process goes, and time its duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948d12c1-c713-4632-bcc6-22f59192f436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m triple_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mcount\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Call doi_getter to get list as many filtered DOI URLs as our sample size\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m doi_list \u001b[38;5;241m=\u001b[39m \u001b[43mdoi_getter_pagination\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1307\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriple_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Write log entry with number of DOIs grabbed for this iteration of the loop\u001b[39;00m\n\u001b[1;32m     35\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+++++++++++++++++++++++++++++++++++++++++++\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m, in \u001b[0;36mdoi_getter_pagination\u001b[0;34m(country, year, field, paper_num)\u001b[0m\n\u001b[1;32m     21\u001b[0m url \u001b[38;5;241m=\u001b[39m url_builder(country,year,field,page_code)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Perform API call and decode JSON result to obtain meta-data\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m meta_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Extract DOIs from API response and add to doi_lst\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Extract DOIs from API response and add to doi_lst\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[0;32m--> 747\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/urllib3/response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/urllib3/response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[0;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/urllib3/response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/site-packages/urllib3/response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/http/client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[0;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Open log file\n",
    "with open(\"../data/logs/\" + output_file_name + \"logfile.txt\", \"w\") as f:\n",
    "\n",
    "    # Write introductory message into log\n",
    "    f.write(\"Log file opened.\\n\")\n",
    "    \n",
    "    # Store start time of loop execution\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize list to store dois\n",
    "    final_doi_list = []\n",
    "\n",
    "    # For loop to gather dois\n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        # Intialize variables with values from input bucketing specifications\n",
    "        year = str(row['year'])\n",
    "        country = str(row['country'])\n",
    "        count = row['count']\n",
    "        triple_count = 3*count\n",
    "\n",
    "        # Call doi_getter to get three times as many DOI URLs as retraced papers from that bucket\n",
    "        doi_list = doi_getter(country, year, \"1307\", triple_count)\n",
    "    \n",
    "        # Write log entry with number of DOIs grabbed for this iteration of the loop\n",
    "        f.write(\"+++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        f.write(f\"LOOP NUMBER {index + 1}: Year={year}, COUNTRY={country}, COUNT={count}. \\n\")\n",
    "        f.write(f\"Picked {len(doi_list)} DOIs, triple count was {triple_count}. \\n\")\n",
    "        f.write(f\"Of those, {len(set(doi_list))} were unique DOIs \\n\\n\")\n",
    "\n",
    "        # Obtain size of upsized sample from upsize factor defined at the beginning\n",
    "        upsized_sample = int(count * upsize_factor)\n",
    "    \n",
    "        # Randomly get a number of dois equal to a slightly upsized sample size\n",
    "        if len(doi_list) > 0 and len(doi_list) > upsized_sample:\n",
    "            doi_list = pick_random_entries(doi_list, upsized_sample)\n",
    "        else:\n",
    "            doi_list = []\n",
    "        \n",
    "        # Writing log entry with number of DOIs randomly grabbed at this point\n",
    "        f.write(f\"I then picked {len(doi_list)} DOIs randomly. \\n\")\n",
    "        f.write(f\"Of these, {len(set(doi_list))} were unique DOIs. \\n\")\n",
    "        f.write(f\"Target count was {count}.\\n\")\n",
    "        \n",
    "        if len(doi_list) != len(set(doi_list)):\n",
    "            f.write(f\"ERROR: WE HAVE {len(doi_list)} - {len(set(doi_list))} REPEATED DOIS. \\n\")\n",
    "        \n",
    "        # Add result of current iteration to final list of dois\n",
    "        final_doi_list.extend(doi_list) \n",
    "\n",
    "    # Calculate elapsed time for for lopp execution\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_hours, elapsed_minutes, elapsed_seconds = seconds_to_hms(elapsed_time)\n",
    "\n",
    "    print(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {round(elapsed_seconds,1)}s. \\n\")\n",
    "\n",
    "    # Write final message in log file\n",
    "    f.write(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {elapsed_seconds}s. \\n\")\n",
    "    f.write(\"End of log file.\\n\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d339da5-1ae5-4f3c-9268-b05ebcc8e439",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe49e7f-c9bb-4451-98eb-f4d6f4fbd7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert list to data frame\n",
    "\n",
    "df_dois = pd.DataFrame(final_doi_list)\n",
    "\n",
    "# Write data frame to .csv\n",
    "\n",
    "df_dois.to_csv(output_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182b7ed6-e37e-4374-9495-c582366a64e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create set with DOIs to get rid of repeated entries\n",
    "\n",
    "final_doi_set = set(final_doi_list)\n",
    "\n",
    "# Convert set to data frame\n",
    "\n",
    "df_unique_dois = pd.DataFrame(final_doi_set)\n",
    "\n",
    "# Write data frame to .csv\n",
    "\n",
    "df_unique_dois.to_csv(output_path_unique, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1e296-4c33-43c8-b04b-b2eb8b4ebb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python \"04c_extract_nonretract_abstract_as_text.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9dd95-6a11-45cc-a290-9c42cf946ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
