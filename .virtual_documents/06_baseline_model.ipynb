















# File name for text data from fraudulent papers
#file_name_fraudulent = "cellbio_abstracts_non-retracted_text.csv"

file_name_fraudulent = "cellbio_abstracts_retracted_text.csv"

# File path for text data from fraudulent papers

file_path_fraudulent = "../data/text_data/" + file_name_fraudulent

# File name for text data from legitimate papers
#file_name_legit = "cellbio_abstracts_retracted_text.csv"

file_name_legit = "cellbio_abstracts_non-retracted_text.csv"

# File path for text data from legitimate papers

file_path_legit = "../data/text_data/" + file_name_legit 

# File path for country codes dictionary

file_path_country_codes = "../data/country_codes_dictionary.csv"









# Import required libraries

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

import sklearn
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from xgboost.sklearn import XGBClassifier

import json
import os

from scipy.stats import norm

import time
from function_definitions import seconds_to_hms











# Read reconstructed abstracts for retracted papers from .csv file
# Note that separator is "|"

df_fraudulent_abstracts = pd.read_csv(file_path_fraudulent, sep = "|" )

# Read country codes correspondences from .csv

df_country_codes = pd.read_csv(file_path_country_codes, encoding='latin-1')

# Display data frame

df_fraudulent_abstracts.head(1)







# Filter away columns for which "ret_flag" is true

df_fraudulent_abstracts = df_fraudulent_abstracts[df_fraudulent_abstracts['ret_flag'] != True]

df_ret_flag = df_fraudulent_abstracts[df_fraudulent_abstracts['ret_flag'] = True]







# Store column with reconstructed text into our list for the legit corpus

corpus_fraudulent = df_fraudulent_abstracts.abstract_text

# Visualize result

corpus_fraudulent









# Read reconstructed abstracts for non-retracted papers from .csv file
# Note that separator is "|"

df_legit_abstracts = pd.read_csv(file_path_legit, sep = "|" )

# Display data frame

df_legit_abstracts.head(1)








# Filter away columns for which "ret_flag" is true

#df_legit_abstracts = df_legit_abstracts[df_legit_abstracts['ret_flag'] != True]

df_legit_abstracts = df_legit_abstracts[df_legit_abstracts['ret_flag']]

df_legit_abstracts


df_legit_abstracts.at[5972, "abstract_text"]



# Store column with reconstructed text into our list for the legit corpus

corpus_legit = df_legit_abstracts.abstract_text







# Create corpus

corpus = []
corpus.extend(corpus_fraudulent) # Note that we are adding our fraudulent papers first
corpus.extend(corpus_legit)









# Create data frame with all abstracts

df_all_abstracts = pd.concat([df_legit_abstracts, df_fraudulent_abstracts], ignore_index=True)

# Definte target and features

X = df_all_abstracts["abstract_text"]
y = df_all_abstracts["target"]






# Split data into train and text sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)









# Instantiate CountVectorizer 

vectorizer = CountVectorizer()

# Fit vectorizer to corpus of fraudulent papers to create bag of words vectors

X_train_vec = vectorizer.fit_transform(X_train)

# Obtain vocabulary for corpus

print("Vocabulary:")
print(vectorizer.get_feature_names_out())

# Print bag of words vectors for each document in corpus

print("\nBag of Words vectors:")
print(X_train_vec.toarray())






# Vectorize test data set by using our vectorizer

X_test_vec = vectorizer.transform(X_test)









# Instantiate CountVectorizer 

tfidf = TfidfVectorizer()

# Fit vectorizer to corpus of fraudulent papers to create bag of words vectors

X_train_tfidf = tfidf.fit_transform(X_train)

# Obtain vocabulary for corpus

print("Vocabulary:")
print(tfidf.get_feature_names_out())

# Print bag of words vectors for each document in corpus

print("\nTF-IDF vectors:")
print(X_train_tfidf.toarray())






# Vectorize test data set using vocabulary from train

X_test_tfidf = tfidf.transform(X_test)






# Instantiate select k best 

k_best = SelectKBest(score_func=f_classif, k=20)  

# Fit select k best to training data 

X_train_selected_vec = k_best.fit_transform(X_train_vec, y_train)

# Get indices of k most salient feature

selected_feature_indices = k_best.get_support(indices=True)

# Obtain words corresponding to each index from vectorizer

features_list = vectorizer.get_feature_names_out()


# Obtain words corresponding to most salient features from index

selected_feature_names = [features_list[i] for i in selected_feature_indices]

# Visualize most salient words

selected_feature_names

# Obtain scores of selected features
selected_feature_scores = k_best.scores_[selected_feature_indices]

# Pair feature names with their corresponding scores
feature_scores = list(zip(selected_feature_names, selected_feature_scores))

# Print feature names and scores
for feature_name, score in feature_scores:
    print(f"Feature: {feature_name}, Score: {score}")



# Create a boolean mask to filter rows where "assay" is present in the "abstract_text" column
mask = df_all_abstracts["abstract_text"].str.contains("western")

# Apply the mask to filter rows
filtered_rows = df_all_abstracts[mask]

# Display the filtered rows
print(filtered_rows)


df_all_abstracts.iloc[9].abstract_text










# Instantiate logistic regressor

log_reg = LogisticRegression(max_iter = 200)

# Store start time of loop execution

start_time = time.time()

# Fit logistic regressor to our train data set

#log_reg.fit(X_train_vec, y_train)

log_reg.fit(X_train_tfidf, y_train)

# Use logistic regressor to obtain predictions for test data set

#y_pred = log_reg.predict(X_test_vec)

y_pred = log_reg.predict(X_test_tfidf)

# And for train data set

#y_pred_train = log_reg.predict(X_train_vec)

y_pred_train = log_reg.predict(X_train_tfidf)

# Calculate elapsed time for for training model

elapsed_time = time.time() - start_time

# Convert time to hours, minutes, and seconds

elapsed_hours, elapsed_minutes, elapsed_seconds = seconds_to_hms(elapsed_time)

# Print message with elapsed time

print(f"Time taken to train the model: {elapsed_hours}h, {elapsed_minutes}m, {elapsed_seconds}s. \n")







# Obtain accuracy score and other evaluation metrics

accuracy_test = accuracy_score(y_test, y_pred)
accuracy_train = accuracy_score(y_train, y_pred_train)

report_test = classification_report(y_test, y_pred)

report_train = classification_report(y_train, y_pred_train)

# Print scores

print("Accuracy on train data set:", accuracy_train)
print("Accuracy on test data set:", round(accuracy_test,2))

print("Classification Report for Train:\n", report_train)

print("Classification Report for Test:\n", report_test)






# Obtain confusion matrix
conf_matrix_log_reg = confusion_matrix(y_test, y_pred)

print("Confusion Matrix for Log Reg:\n", conf_matrix_log_reg)






# Obtain coefficients for logistic regressor

coefficients = log_reg.coef_[0]
intercept = log_reg.intercept_

# Obtain feature names from vocabulary

feature_names = vectorizer.get_feature_names_out()

# Pair each feature with its coefficient

coefficients_with_features = list(zip(feature_names, coefficients))

# Sort coefficients based on absolute values

sorted_coefficients = sorted(coefficients_with_features, key=lambda x: abs(x[1]), reverse=True)

# Print the top 10 coefficients along with their corresponding features

print("Top 20 coefficients with largest absolute magnitude:")
for i, (feature, coefficient) in enumerate(sorted_coefficients[:15], start=1):
    print(f"{i}. Feature: {feature}, Coefficient: {coefficient}")
    





from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Sort coefficients by absolute value
sorted_coefficients = sorted(sorted_coefficients, key=lambda x: abs(x[1]), reverse=True)

# Select the top 30 words
top_words = sorted_coefficients[:30]

# Separate words with positive and negative coefficients
positive_words = [word for word, coeff in top_words if coeff > 0]
negative_words = [word for word, coeff in top_words if coeff < 0]

# Generate word cloud with custom color function
def fixed_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    if word in positive_words:
        # Set color to light blue for positive coefficients
        return "rgb(135, 206, 250)"  # Light blue color
    elif word in negative_words:
        # Set color to light grey for negative coefficients
        return "rgb(192, 192, 192)"  # Light grey color
    else:
        # Default color for neutral words
        return "rgb(128, 128, 128)"  # Default grey color

# Convert top_words into a dictionary
word_freq_dict = {word: abs(coeff) for word, coeff in top_words}

# Generate word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white', color_func=fixed_color_func)
wordcloud.generate_from_frequencies(word_freq_dict)

# Display the generated word cloud using matplotlib
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()












xgboost = XGBClassifier()





xgboost.fit(X_train_tfidf, y_train)






# Predictions for the test data set

y_pred_xgb = xgboost.predict(X_test_tfidf)

# And for train data set

y_pred_train_xgb = xgboost.predict(X_train_tfidf)







# Obtain accuracy score and other evaluation metrics

accuracy_test_xgb = accuracy_score(y_test, y_pred_xgb)
accuracy_train_xgb = accuracy_score(y_train, y_pred_train_xgb)

report_test_xgb = classification_report(y_test, y_pred_xgb)

report_train_xgb = classification_report(y_train, y_pred_train_xgb)

# Print scores

print("Accuracy on train data set (XGB Classifier):", accuracy_train_xgb)
print("Accuracy on test data set (XGB Classifier):", round(accuracy_test_xgb,2))

print("Classification Report for Train (XGB Classifier):\n", report_train_xgb)

print("Classification Report for Test (XGB Classifier):\n", report_test_xgb)






# Obtain confusion matrix
conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)

print("Confusion Matrix XGB Classifier:\n", conf_matrix_xgb)







# Configuration of the RandomForest Classifier
clf = RandomForestClassifier(
    n_estimators=8000,
    max_depth=20,
    max_features='sqrt',
    random_state=42,
    n_jobs=-1
)

# Start timing for fitting the model
start_time = time.time()
clf.fit(X_train_tfidf, y_train)
end_time = time.time()
fit_time = end_time - start_time
hours, minutes, seconds = seconds_to_hms(fit_time)
print(f"Training completed in: {hours} hours, {minutes} minutes, and {seconds:.2f} seconds")

# Start timing for making predictions
start_time = time.time()
y_pred_rf = clf.predict(X_test_tfidf)
y_pred_train_rf = clf.predict(X_train_tfidf)
end_time = time.time()
predict_time = end_time - start_time
hours, minutes, seconds = seconds_to_hms(predict_time)
print(f"Prediction completed in: {hours} hours, {minutes} minutes, and {seconds:.2f} seconds")

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred_rf)
print("Accuracy:", accuracy)



# Get feature importances from the trained model
importances = clf.feature_importances_

# Get feature names from the vectorizer
feature_names = tfidf.get_feature_names_out()

# Define the number of top features you want to display
K = 20

# Get the indices of the K most important features in descending order of importance
indices = importances.argsort()[-K:][::-1]

# Display the top K important features with their names and importance scores
top_features = [(feature_names[i], importances[i]) for i in indices]
for feature, importance in top_features:
    print(f"{feature}: {importance:.4f}")



# Define function to fit & rpedict random forest


def train_and_log_rf(n_estimators, max_depth, train_data, train_labels, test_data, test_labels):
    # Configuration of the RandomForest Classifier
    clf = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1
    )

    # Start timing for fitting the model
    start_time = time.time()
    clf.fit(train_data, train_labels)
    fit_time = time.time() - start_time  # Time in seconds
    print(f"Training Time: {fit_time:.2f} seconds")

    # Start timing for making predictions
    start_time = time.time()
    y_pred = clf.predict(test_data)
    predict_time = time.time() - start_time  # Time in seconds
    print(f"Prediction Time: {predict_time:.2f} seconds")

    # Evaluate the model
    accuracy = accuracy_score(test_labels, y_pred)
    print("Accuracy:", accuracy)

    # Prepare data for logging
    log_data = {
        'n_estimators': n_estimators,
        'max_depth': max_depth,
        'training_time': f"{fit_time:.2f} seconds",
        'prediction_time': f"{predict_time:.2f} seconds",
        'accuracy': accuracy
    }
    log_df = pd.DataFrame([log_data])

    # Define log file path
    log_file_path = '../data/model_logs/model_performance_log.csv'

    # Check if the directory exists, if not, create it
    os.makedirs(os.path.dirname(log_file_path), exist_ok=True)

    # Check if file exists, append if it does, otherwise write a new file
    if os.path.isfile(log_file_path):
        log_df.to_csv(log_file_path, mode='a', header=False, index=False)
    else:
        log_df.to_csv(log_file_path, mode='w', header=True, index=False)

    # Return the model, predictions, and accuracy
    return clf, y_pred, accuracy




# Define number of trees & tree depth and instatiate fitting the model and making the predicitons

num_trees = 4000
tree_depth = 40

clf, y_pred_rf, accuracy_rf = train_and_log_rf(num_trees, tree_depth, X_train_tfidf, y_train, X_test_tfidf, y_test)




# Calculate accuracy just for train

accuracy_train_rf = accuracy_score(y_train, y_pred_train_rf)
print(f"Accuracy: {accuracy_train_rf:.2f}")

# Generate and print classification report for train

report_train_rf = classification_report(y_train, y_pred_train_rf)
print("Classification Report:")
print(report_train_rf)



# Calculate accuracy just for test

accuracy_test_rf = accuracy_score(y_test, y_pred_rf)
print(f"Accuracy: {accuracy_test_rf:.2f}")

# Generate and print classification test

report_test_rf = classification_report(y_test, y_pred_rf)
print("Classification Report:")
print(report_test_rf)


# Assign
cm = confusion_matrix(y_test, y_pred_rf)


# Plotting the heatmap
plt.figure(figsize=(5, 3))  # Adjust size as needed
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)  # 'fmt' is a string format code
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()









# Obtain predicted probabilities for positive class (Log Reg)

y_pred_proba = log_reg.predict_proba(X_test_vec)[:, 1]

# Obtain predicted probabilities for positive class (XGB classifier)

y_pred_proba_xgb = xgboost.predict_proba(X_test_vec)[:, 1]

# Obtain predicted probabilities for positive class (random forest classifier)

y_pred_proba_rf = clf.predict_proba(X_test_vec)[:, 1]

# Compute ROC curve and ROC area for each class

fpr, tpr, _ = roc_curve(y_test, y_pred_proba)

fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_pred_proba_xgb)

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_proba_rf)

# Compute Area Under the Curve (AUC)

roc_auc = auc(fpr, tpr)

roc_auc_xgb = auc(fpr_xgb, tpr_xgb)

roc_auc_rf = auc(fpr_rf, tpr_rf)


# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='orange', lw=2, label='Logistic Regressor (area = %0.2f)' % roc_auc)
plt.plot(fpr_xgb, tpr_xgb, color='lightgreen', lw=2, label='XG Boost Classifier (area = %0.2f)' % roc_auc_xgb)
plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label='Random Forest Classifier (area = %0.2f)' % roc_auc_rf)

plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()











# Obtain only abstracts for papers from China and store them in data frame

df_ch = df_all_abstracts[df_all_abstracts["country"] == "CN"]

# Visualize shape of filtered array

print(df_ch.shape)

# Vectorize text for those papers using TF-IDF vectorization

X_ch_tfidf = tfidf.transform(df_ch["abstract_text"])

# Obtain predictions for vectorized text with logistic reg model

y_pred_ch = log_reg.predict(X_ch_tfidf)



# Obtain number of papers flagged as fraudulent

y_pred_ch.sum()



# Obtain number of papers that we tested in total

len(y_pred_ch)



# Compute percentage of papers flagged as fraudulent for China

per_fraud_china = y_pred_ch.sum() / len(y_pred_ch) * 100

# Visualize result

per_fraud_china 


df_ch_fraud = df_ch[df_ch["target"] == 1]


df_ch_fraud.shape


df_ch_fraud.shape[0] / df_ch.shape[0] * 100






# Obtain only abstracts for papers from China and store them in data frame

df_us = df_all_abstracts[df_all_abstracts["country"] == "US"]

# Visualize shape of filtered array

print(df_us.shape)

# Vectorize text for those papers using TF-IDF vectorization

X_us_tfidf = tfidf.transform(df_us["abstract_text"])

# Obtain predictions for vectorized text with logistic reg model

y_pred_us = log_reg.predict(X_us_tfidf)



# Compute percentage of papers flagged as fraudulent for China

per_fraud_us = y_pred_us.sum() / len(y_pred_us) * 100

# Visualize result

per_fraud_us


df_us_fraud = df_us[df_us["target"] == 1]


df_us_fraud.shape[0] / df_us.shape[0] * 100






def model_tester_country(country_code, df, vectorizer, model):
    
    # Obtain only abstracts for papers from China and store them in data frame

    df_filtered = df[df["country"] == country_code]

    # Vectorize text for those papers using TF-IDF vectorization

    X_filtered = vectorizer.transform(df_filtered["abstract_text"])

    # Obtain predictions for vectorized text with logistic reg model

    y_pred_filtered = model.predict(X_filtered)
    
    # Obtain total number of papers flagged
    
    total_flagged = y_pred_filtered.sum()

    # Compute percentage of papers flagged as fraudulent for country

    percentage_flagged = total_flagged / len(y_pred_filtered) * 100
    
    # Calculate total number of papers that are actually fraudulent for country
    
    total_fraud = df_filtered[df_filtered["target"] == 1].shape[0]
    
    # Calculate percentage of papers that are actually fraudulent for country
    
    percentage_fraud = total_fraud / df_filtered.shape[0] * 100

    # Print results
    
    #print(f"Perfectage of papers flagged for {country_code} {percentage_flagged}")
    #print(f"Perfectage of fraudulent papers for {country_code} {percentage_fraud}")
    
    return percentage_flagged, percentage_fraud, total_flagged, total_fraud
 






# Calculate percentage of papers flagged for US

model_tester_country("US", df_all_abstracts, tfidf, log_reg)



# Now for China

model_tester_country("CN", df_all_abstracts, tfidf, log_reg)



# UK

model_tester_country("GB", df_all_abstracts, tfidf, log_reg)



# India

model_tester_country("IN", df_all_abstracts, tfidf, log_reg)



# Iran

model_tester_country("IR", df_all_abstracts, tfidf, log_reg)


# France

model_tester_country("AU", df_all_abstracts, tfidf, log_reg)






def model_tester_year(year, df, vectorizer, model):
    
    # Obtain only abstracts for papers from China and store them in data frame

    df_filtered = df[df["year"] == year]

    # Vectorize text for those papers using TF-IDF vectorization

    X_filtered = vectorizer.transform(df_filtered["abstract_text"])

    # Obtain predictions for vectorized text with logistic reg model

    y_pred_filtered = model.predict(X_filtered)

    # Obtain total number of papers flagged
    
    total_flagged = y_pred_filtered.sum()

    # Compute percentage of papers flagged as fraudulent for country

    percentage_flagged = total_flagged / len(y_pred_filtered) * 100
    
    # Calculate total number of papers that are actually fraudulent for country
    
    total_fraud = df_filtered[df_filtered["target"] == 1].shape[0]
    
    # Calculate percentage of papers that are actually fraudulent for country
    
    percentage_fraud = total_fraud / df_filtered.shape[0] * 100
    
    return percentage_flagged, percentage_fraud, total_flagged, total_fraud
 


model_tester_year(2020, df_all_abstracts, tfidf, log_reg)


model_tester_year(2019, df_all_abstracts, tfidf, log_reg)


model_tester_year(2009, df_all_abstracts, tfidf, log_reg)






def model_tester_year_country(year, country, df, vectorizer, model):
    
    # Obtain only abstracts for papers from a given bucket and store them in data frame

    df_filtered = df[(df["year"] == year) & (df["country"] == country)]

    # Vectorize text for those papers using our vectorizer

    X_filtered = vectorizer.transform(df_filtered["abstract_text"])

    # Obtain predictions for vectorized text with our model

    y_pred_filtered = model.predict(X_filtered)

    # Obtain total number of papers flagged
    
    total_flagged = y_pred_filtered.sum()

    # Compute percentage of papers flagged as fraudulent for country

    percentage_flagged = total_flagged / len(y_pred_filtered) * 100
    
    # Calculate total number of papers that are actually fraudulent for country
    
    total_fraud = df_filtered[df_filtered["target"] == 1].shape[0]
    
    # Calculate percentage of papers that are actually fraudulent for country
    
    percentage_fraud = total_fraud / df_filtered.shape[0] * 100

    # Print results
    
    #print(f"Perfectage of papers flagged for {year} and {country} {percentage_flagged}")
    #print(f"Perfectage of fraudulent papers for {year} and {country} {percentage_fraud}")
    
    return percentage_flagged, percentage_fraud, total_flagged, total_fraud
    



model_tester_year_country(2017, "CN", df_all_abstracts, tfidf, log_reg)



model_tester_year_country(2021, "US", df_all_abstracts, tfidf, log_reg)



model_tester_year_country(2018, "IR", df_all_abstracts, tfidf, log_reg)






countries = df_all_abstracts["country"].unique()
results = []
difference_percentage = []
difference_total = []

for country in countries:
    country_results = model_tester_country(country, df_all_abstracts, tfidf, log_reg)
    results.append(country_results)
    difference_percentage.append(country_results[0] - country_results[1])
    difference_total.append(country_results[2] - country_results[3])






df_differences = pd.DataFrame({"country": countries, "percentage_difference": difference_percentage, "total_difference": difference_total})



df_differences = df_differences[df_differences["total_difference"] != 0]



df_differences.plot.bar(x="country", y="total_difference")




# Obtain size of data frame to decide how to filter top entires

df_differences.shape



# Filter data frame to obtain countries with most papers in absolute number only 

df_differences_filtered = df_differences.head(27)



# Make normal plot

df_differences.plot.bar(x="country", y="percentage_difference")




# Make plot for filtered data frame

ax = df_differences_filtered.plot.bar(x="country", y="percentage_difference", color='grey', legend=False)

# Set the y axis to -50 and 50 percentage points

ax.set_ylim(-70, 70)

# Add grid

ax.grid(True)

# Display plot

plt.show()







year_results = model_tester_year(2020, df_all_abstracts, tfidf, log_reg)



years = df_all_abstracts["year"].unique()
results_year = []
difference_percentage_year = []
difference_total_year = []


    
for year in years:
    year_results = model_tester_year(year, df_all_abstracts, tfidf, log_reg)
    results_year.append(year_results)
    difference_percentage_year.append(year_results[0] - year_results[1])
    difference_total_year.append(year_results[2] - year_results[3])




df_differences_year = pd.DataFrame({"year": years, "percentage_difference": difference_percentage_year, "total_difference": difference_total_year})


df_differences_year.head(5)


df_differences_year.plot.bar(x="year", y="percentage_difference")




df_differences_year.plot.bar(x="year", y="total_difference")







df_china = df_all_abstracts[df_all_abstracts["country"] == "CN"]

# Vectorize text for those papers using TF-IDF vectorization

X_china = tfidf.transform(df_china["abstract_text"])

# Obtain predictions for vectorized text with logistic reg model

y_pred_china = log_reg.predict(X_china)
    


y_china = df_china["target"]

report_test = classification_report(y_china, y_pred_china)

print("Classification Report for China:\n", report_test)


conf_matrix_china = confusion_matrix(y_china, y_pred_china)

conf_matrix_china






df_us = df_all_abstracts[df_all_abstracts["country"] == "US"]

# Vectorize text for those papers using TF-IDF vectorization

X_us = tfidf.transform(df_us["abstract_text"])

# Obtain predictions for vectorized text with logistic reg model

y_pred_us = log_reg.predict(X_us)
    


y_us = df_us["target"]

report_test_us = classification_report(y_us, y_pred_us)

print("Classification Report for the USA:\n", report_test_us)


conf_matrix_us = confusion_matrix(y_us, y_pred_us)

conf_matrix_us






