















# File path for input .csv file

input_path = "../data/retraction_watch_data_set/1_original_data_set.csv"







# File path for output .csv file

output_path = "../data/retraction_watch_data_set/2_clean_data_set.csv"










# Import required libraries

import numpy as np
import pandas as pd
import missingno as msno







# Read data from .csv file 

df = pd.read_csv(input_path, encoding='latin-1')










# Display header 

df.head(1)






df.shape






# Display info on data type and non-null entries per column

df.info()







# Change column names to snake case

df.columns = [col.lower().replace(" ", "_") for col in df.columns]

# Check that it worked

df.columns







# Change names of remaining columns to snake_case

df = df.rename(columns={'articletype': 'article_type'})

df = df.rename(columns={'retractiondate': 'retraction_date'})

df = df.rename(columns={'retractiondoi': 'retraction_doi'})

df = df.rename(columns={'retractionpubmedid': 'retraction_pubmed_id'})

df = df.rename(columns={'originalpaperdate': 'original_paper_date'})

df = df.rename(columns={'originalpaperdoi': 'original_paper_doi'})

df = df.rename(columns={'originalpaperpubmedid': 'original_paper_pubmed_id'})

df = df.rename(columns={'retractionnature': 'retraction_nature'})







# Visualize column names again

df.columns







# Visualize NaNs per column using bars

msno.bar(df)

# Visualize NaNs per column using matrix

msno.matrix(df)













# Obtain total number of papers with no doi

df.original_paper_doi.isnull().sum()







# Define percentage_finder function

def percentage_finder(df, col_name, val_name):
    """This function takes a data frame, the name of a column, and one of the 
    possible values of the column. It then returns how often that one value 
    occurs in that column of the data frame."""
    
    if pd.isna(val_name):  # In case we want to check how many entries are NanNs
        count = df[col_name].isna().sum()
    else:
        count = df[df[col_name] == val_name][col_name].count()
        
    percentage = (count / len(df[col_name])) * 100
            
    return percentage






# Call percentage_finder function to find out % of paper with NaN values in "original_paper_doi" column

print(round(percentage_finder(df, "original_paper_doi", np.nan),2), "%")
   






# Create new clean data frame to store clean information only

df_clean = df.copy()

# Drop values with NaN entries from "original_paper_doi" column

df_clean = df_clean.dropna(subset=["original_paper_doi"])

# Check that it worked

df_clean.original_paper_doi.isnull().sum()







# Convert column entries to string string type and strip whitespaces

df_clean['original_paper_doi'] = df_clean['original_paper_doi'].astype(str).str.strip()

# Create new data frame with invalid DOIs that do NOT start with "10." only

df_invalid_dois = df_clean[~df_clean['original_paper_doi'].str.startswith('10.')]

# Visualize resulting data frame with invalid DOIs

df_invalid_dois.head(1)







# Create new data frame with number of occurrances of each kind of invalid DOI as column entries 

invalid_doi_count = df_invalid_dois['original_paper_doi'].value_counts()

# Visualize elements and values of new data frame to find out frequency of each kind of invalid DOI

invalid_doi_count







# Message clarifying meaning of numbers to be displayed

print("Percentage of papers with each kind of invalid DOI:\n")

# Visualize elements and values of invalid_doi_count series 

for element, value in invalid_doi_count.items():
    print(f"{element}: {round(percentage_finder(df_clean,'original_paper_doi', element),3)}%")

 






# Keep only those rows whose value for the "original_doi_column" starts with "10."

df_clean = df_clean[df_clean['original_paper_doi'].str.startswith('10.')]

# Make sure size of clean data frame is correct

print(df_clean.shape)

# Visualize new clean data set to make sure all is as desired

df_clean.head(1)







df['original_paper_doi'] = df['original_paper_doi'].astype(str).fillna('')









# Visualize unique values of the column

df_clean.retraction_nature.unique()







# Define percentage_printer function

def percentage_printer(df, col_name):
    """This function takes a data frame and a column name as input. It then calls
    the percentage_finder function to print the percentage of occurrances of each
    value of the column."""
        
    val_list = []
    val_list = df[col_name].unique()
        
    for value in val_list:
        percentage = percentage_finder(df, col_name, value)
        print(f"{value}: {round(percentage,2)}%")
        






# Define histogram printer function

def histogram_printer(df, col_name):
    """Takes a data frame and a column name as input, produces a histogram
    for the vaulues of that column of the data frame"""
    
    df[col_name].hist()







# Call percentage_printer to obtain frequencies of each reason for retraction

percentage_printer(df_clean, "retraction_nature")







# Call histogram_printer to produce histogram for "Retraction Nature" column

histogram_printer(df_clean, "retraction_nature")







# Retain only entries with "Retraction" value in the "Retraction Nature" column

df_clean = df_clean[df_clean["retraction_nature"] == "Retraction"]

# Drop "Retraction Nature" column

df_clean.drop("retraction_nature", axis=1, inplace = True)

# Check that it worked

df_clean.columns










# Visualize the unique values of the Paywalled column

df_clean.paywalled.unique()







# Call percentage_printer function to visualize relative frequency of each reason

percentage_printer(df_clean, "paywalled")







# Visualize histogram for Paywalled column

histogram_printer(df_clean, "paywalled")













# Display number of unique values in "Article Type" column

df_clean.article_type.nunique()







# Find percentages of articles with each article type

percentage_printer(df_clean, "article_type")







# Drop articles types that are of no interest
# Note the semicolon at the end to match the format of the entries in the "Article Type" column

df_clean = df_clean[df_clean["article_type"].isin(["Research Article;", "Review Article;", "Conference Abstract/Paper;"])]

# Check that it worked

df_clean.article_type.unique()










# Convert column entries of clean data frame to datetime format
# Note that item at position 5045 has diverging format

df_clean['original_paper_date'] = pd.to_datetime(df_clean['original_paper_date'], format='mixed')

# Extract year iformation and create new column with it 

df_clean['year'] = df_clean['original_paper_date'].dt.year

# Visualize result

df_clean.head(1)







# Print message with number of papers after data cleaning

print(f"Number of papers in clean data frame: {df_clean.shape[0]}.")

# Print message with percentage of dropped papers 

print(f"Percentage of papers of original data frame that remain in clean data frame: {round(df_clean.shape[0] / df.shape[0] * 100,2)}%.")










# Save date frame as .csv

df.to_csv(output_path, index=False)

