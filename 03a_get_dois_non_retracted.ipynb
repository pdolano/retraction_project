{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5dd072-bb10-4932-8598-5d3aecba0e48",
   "metadata": {},
   "source": [
    "# 3a. Getting DOIs of Non-Retracted Papers with Specified Country and Year Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cd73e-5aff-4763-b120-1882a4896458",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af693019-1299-4c1c-a455-28cca2f91a78",
   "metadata": {},
   "source": [
    "This Notebook **obtains DOIs of non-retracted papers**. The papers with these DOIs, furthermore, will be **randomly** selected from OpenAlex, and they will have the **same country and year distribution** as the retracted papers in our original data set. Needless to say, they will also be papers from the same discipline.\n",
    "\n",
    "The Notebook takes as input the .csv file that was generated by **Notebook 2c**, contained the country and year distribution of our retracted papers. \n",
    "\n",
    "The **workflow** of the notebook is therefore as follows:\n",
    "\n",
    "- Input: **one .csv file** with the country and year distribution of our retracted papers.\n",
    "- Output: **one .csv file** with the DOIs of non-retracted papers that we got from OpenAlex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db77eb-9d5d-4ebc-8b74-f27acd4eb7dd",
   "metadata": {},
   "source": [
    "## Input / Output Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c809d-2f22-4af8-a683-9a9685d44889",
   "metadata": {},
   "source": [
    "Input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db710a39-004d-4279-868e-d3469f5048cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# File path for .csv input file with bucketing specifications\n",
    "#input_path = \"../data/Country_Year_Buckets_Cellbio.csv\"\n",
    "\n",
    "# Id of subfield to fitler our paper search \n",
    "# Id value for cell_bio is 1307\n",
    "subfield_filter_value = \"1307\"\n",
    "\n",
    "# File path for ISO country code equivalences\n",
    "input_path_dictionary = \"../data/country_code_dictionary.csv\"\n",
    "\n",
    "# Upsize factor for each bucket\n",
    "upsize_factor = 1.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effe7e7-ce8f-4521-9936-ab38f8e5cc1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Output parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b05e7dbf-cc3e-4f83-8cb6-4081153c4102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Paths for previous draft\n",
    "\n",
    "# File name for .csv with DOIs of non-retracted papers\n",
    "#output_file_name = \"dois_jenny_corrected_3.csv\"\n",
    "\n",
    "# File path for .csv with unique DOIs of non-retracted papers\n",
    "#output_path_unique = \"../data/results/dois_jenny_unique.csv\"\n",
    "\n",
    "# File path for .jsonl file with text data for abstracts\n",
    "#output_path = \"../data/results/\" + output_file_name\n",
    "\n",
    "# Current path\n",
    "\n",
    "# File path for .csv with DOIs of non-retracted papers\n",
    "\n",
    "output_path = \"../data/dois_non_retracted/non_retracted_dois_cell_bio.csv\" \n",
    "\n",
    "# File path for log file\n",
    "\n",
    "output_path_log = \"../data/logs/non_retracted_dois_getting_log.txt\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45085f7-21d4-460c-8d59-fd8a28ac73fb",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989ee18-aab1-4569-b140-f84b86beed73",
   "metadata": {},
   "source": [
    "\n",
    "As always, let's start by importing all required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea4fa866-c999-433d-bb8f-0eabffc89178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from json.decoder import JSONDecodeError\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68239fe-4b98-4391-a0f5-23f8347e3baa",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed31b87-d76c-46ae-9bd6-78105f9234ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "And by loading the relevant data from our .csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b908b82-7ce1-448f-a292-a2a4b3521e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load .csv with bucketing specifications into data frame\n",
    "df = pd.read_csv(input_path, encoding='latin-1', sep = \";\")\n",
    "\n",
    "# Load .csv file with ISO country code equivalences\n",
    "df_country_codes = pd.read_csv(\"../data/country_codes_dictionary.csv\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215177a-6c7b-4101-a83c-f96e9c69c4df",
   "metadata": {},
   "source": [
    "The data specifying our contry code equivalences requires some cleaninig, so we will go ahead and make the necessary adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c10835b3-a832-4792-a8df-b0d1987d76db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Clean spurious spaces in \"Country\" column\n",
    "df_country_codes['Country'] = df_country_codes['Country'].str.strip()\n",
    "\n",
    "# Clean spurious spaces in \"TIS\" column\n",
    "df_country_codes['TIS'] = df_country_codes['TIS'].str.strip()\n",
    "\n",
    "# Create country code dictionary from countr code data frame\n",
    "country_codes_dictionary = df_country_codes.set_index('Country')['TIS'].str.strip().to_dict()\n",
    "\n",
    "# Rename country column \n",
    "df.rename(columns={\"ï»¿country\": \"country\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3916d0-ffea-4a49-91a7-de6a6364ce8a",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21699f0e-b65c-4f39-995e-05af825570b0",
   "metadata": {},
   "source": [
    "\n",
    "Our goal in this Notebook will consist of obtaining a list of DOIs for non-retracted papers. So as to avoid biases in our model, we will make sure that these papers belong to the same discipline, and have the same country and year distribution as our retracted papers (recall, of course, that we obtained the country and year \"buckets\" for these papers in a previous Notebook). \n",
    "\n",
    "To do this, we will query OpenAlex for papers with these characteristics. This, in turn, is done by making an API call to an URL address that incorporates the appropriate filters. The first step in our task, therefore, will be to define a function that builds an OpenAlex-compatible URL, given a number of paper characteristics that we want to filter our search for:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65f42aeb-139c-4a79-842f-7b34dd22ef82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define url_builder function\n",
    "\n",
    "def url_builder(country, year, field_id, page = \"1\"):\n",
    "    \"\"\"\n",
    "    Builds an URL that can be used to search papers with appropriate filters on OpenAlex \n",
    "    \n",
    "    Parameters: \n",
    "        country (str): country of papers to be queried\n",
    "        year (str): year of papers to be queried\n",
    "        field_id (str): id code of field of papers\n",
    "        page (str): number of page from which papers will be queried (in case we need more papers than can fit in a single \"page\")\n",
    "    \n",
    "    Returns:\n",
    "        url (str): full url to be used for OpenAlex API query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add subfield filter to our URL\n",
    "    subfield_filter = \"primary_topic.subfield.id:\" + field_id    \n",
    "        \n",
    "    # Add publication year filter to our URL\n",
    "    year_filter = \"publication_year:\" + year\n",
    "\n",
    "    # Add retraction filter to make sure queried papers are NOT retracted\n",
    "    retraction_filter = \"is_retracted:false\"\n",
    "    \n",
    "    # Add country filter to our URL\n",
    "    country_code = country\n",
    "    country_filter = \"institutions.country_code:\" + country_code\n",
    "    \n",
    "    # Add type of work filter\n",
    "    type_filter = \"type:article\"\n",
    "    \n",
    "    # Add page number\n",
    "    page_number = page\n",
    "    \n",
    "    # Add filters to base url\n",
    "    url = \"https://api.openalex.org/works?page=\" + page_number + \"&per-page=200&filter=\" + subfield_filter + \",\" + year_filter + \",\" + country_filter + \",\" + type_filter + \",\" + retraction_filter\n",
    "\n",
    "    # Return full URL\n",
    "    return url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde007b6-b9fb-4304-9d0d-83ed66c627bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our next step will consist of obtaining a specified number of DOI addresses for the appropriately filtered papers that our URLs can query from OpenAlex. We will define a new function in order to do that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b81958a-d689-4daa-9508-cb515986ea35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Master function\n",
    "# Define doi_getter function\n",
    "\n",
    "def doi_getter(country, year, field, paper_num):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts DOIs of papers based on specified criteria.\n",
    "\n",
    "    Parameters:\n",
    "        country (str): The country associated with the papers.\n",
    "        year (str): The year of publication of the papers.\n",
    "        field (str): The field or discipline of the papers.\n",
    "        paper_num (int): The number of DOIs to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list containing the extracted DOIs of the papers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize list to store DOIs\n",
    "    doi_lst = []  \n",
    "    \n",
    "    # Calculate the number of pages require to fetch the specified number of papers\n",
    "    page_num = int(paper_num / 200) + 1\n",
    "    \n",
    "    # Perform API calls until the desired number of DOIs is collected\n",
    "    for page in range(1, page_num + 1):\n",
    "\n",
    "        # Update search URL with page number\n",
    "        page_code = str(page)\n",
    "        url = url_builder(country,year,field,page_code)\n",
    "        \n",
    "        # Perform API call and decode JSON result to obtain meta-data\n",
    "        response = requests.get(url)\n",
    "        meta_data = response.json()\n",
    "                \n",
    "        # Extract DOIs from API response and add to doi_lst\n",
    "        for element in meta_data[\"results\"]:\n",
    "            if element[\"doi\"] is not None:\n",
    "                doi_lst.append(element[\"doi\"])\n",
    "            if len(doi_lst) >= paper_num:\n",
    "                return list(set(doi_lst))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ec496-387d-452e-81f4-c2cffba6e915",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Note of course that it is possible that our our API call to access information from OpenAlex may fail or that other complications may arise in our attempts to get DOIs for non-retracted papers. In order to get around this problem, we will try to collect more DOIs of non-retracted papers than we need from each country and year \"bucket.\" In fact, we will make this quantity substantially larger so as to make sure that we always get as many DOIs for non-retracted papers as we need. Of course, this means that we will typically get more DOIs than necessary for each country and year bucket. Whenever this happens, we will randomly select as many DOIs as we actually need from the ones that we obtained.\n",
    "\n",
    "We will therefore need a function to pick a specified number of elements of a given list at random, which we can go on to define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "534ea179-7bb3-4a5e-8075-680c93e70df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define pick_random_entries function\n",
    "\n",
    "def pick_random_entries(input_list, number):\n",
    "    \"\"\"\n",
    "    Picks a specified number of random elements from a list\n",
    "    \n",
    "    Parameters:\n",
    "        input_list (list): The list with elements to be picked at random\n",
    "        number (int): Number of elements to be picked at random\n",
    "    \n",
    "    Returns:\n",
    "        (list): List with specified number of elements picked at random\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate n random indices within the range of the string length\n",
    "    random_indices = random.sample(range(len(input_list)), number)\n",
    "    \n",
    "    # Select the elements at the random indices\n",
    "    random_elements = [input_list[i] for i in random_indices]\n",
    "    \n",
    "    return random_elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a2671-bf2f-4247-9d1b-ad93298d3a60",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Finally, and since we will be obtaining DOIs for a considerable number of papers, it will  be useful to have a sense of how each attempted at downloading information takes. In order to be able to present that information in a more readable format, we will define a function that converts an amount of time expressed in seconds, to the same amount expressed in hours, minutes, and seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5359f6a-e317-40d8-bdec-040a93684803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define seconds_to_hms function\n",
    "\n",
    "def seconds_to_hms(seconds):\n",
    "    \"\"\"\n",
    "    Convert seconds to hours, minutes, and seconds.\n",
    "\n",
    "    Parameters:\n",
    "        seconds (int): Number of seconds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the equivalent time in hours, minutes, and seconds.\n",
    "    \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "\n",
    "    return hours, minutes, seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c274b-0d80-41a4-9eb4-d23a1036b9e6",
   "metadata": {},
   "source": [
    "## Getting DOIs of Non-Retracted Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975fc6f-dc1d-4863-b45a-f8f370928abf",
   "metadata": {},
   "source": [
    "We can now make use of these functions to obtain the desired number of DOIs for non-retracted papers for each year and country. We will also write a log documenting how the download process advances and how many DOIs are obtained for each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "948d12c1-c713-4632-bcc6-22f59192f436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for the loop: 0.0h, 17.0m, 42.7s. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open log file\n",
    "with open(\"output_path_log\", \"w\") as f:\n",
    "\n",
    "    # Write introductory message into log\n",
    "    f.write(\"Log file opened.\\n\")\n",
    "    \n",
    "    # Store start time of loop execution\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize list to store dois\n",
    "    final_doi_list = []\n",
    "\n",
    "    # For loop to gather dois\n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        # Intialize variables with values from input bucketing specifications\n",
    "        year = str(row['year'])\n",
    "        country = str(row['country'])\n",
    "        count = row['count']\n",
    "        triple_count = 3*count\n",
    "\n",
    "        # Call doi_getter to get three times as many DOI URLs as retraced papers from that bucket\n",
    "        doi_list = doi_getter(country, year, \"1307\", triple_count)\n",
    "    \n",
    "        # Write log entry with number of DOIs grabbed for this iteration of the loop\n",
    "        f.write(\"+++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        f.write(f\"LOOP NUMBER {index + 1}: Year={year}, COUNTRY={country}, COUNT={count}. \\n\")\n",
    "        f.write(f\"Picked {len(doi_list)} DOIs, triple count was {triple_count}. \\n\")\n",
    "        f.write(f\"Of those, {len(set(doi_list))} were unique DOIs \\n\\n\")\n",
    "\n",
    "        # Obtain size of upsized sample from upsize factor defined at the beginning\n",
    "        upsized_sample = int(count * upsize_factor)\n",
    "    \n",
    "        # Randomly get a number of dois equal to a slightly upsized sample size\n",
    "        if len(doi_list) > 0 and len(doi_list) > upsized_sample:\n",
    "            doi_list = pick_random_entries(doi_list, upsized_sample)\n",
    "        else:\n",
    "            doi_list = []\n",
    "        \n",
    "        # Writing log entry with number of DOIs randomly grabbed at this point\n",
    "        f.write(f\"I then picked {len(doi_list)} DOIs randomly. \\n\")\n",
    "        f.write(f\"Of these, {len(set(doi_list))} were unique DOIs. \\n\")\n",
    "        f.write(f\"Target count was {count}.\\n\")\n",
    "        \n",
    "        if len(doi_list) != len(set(doi_list)):\n",
    "            f.write(f\"ERROR: WE HAVE {len(doi_list)} - {len(set(doi_list))} REPEATED DOIS. \\n\")\n",
    "        \n",
    "        # Add result of current iteration to final list of dois\n",
    "        final_doi_list.extend(doi_list) \n",
    "\n",
    "    # Calculate elapsed time for for lopp execution\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_hours, elapsed_minutes, elapsed_seconds = seconds_to_hms(elapsed_time)\n",
    "\n",
    "    print(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {round(elapsed_seconds,1)}s. \\n\")\n",
    "\n",
    "    # Write final message in log file\n",
    "    f.write(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {elapsed_seconds}s. \\n\")\n",
    "    f.write(\"End of log file.\\n\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d339da5-1ae5-4f3c-9268-b05ebcc8e439",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c4dd3-835a-4b34-8324-fa2feeb4624b",
   "metadata": {},
   "source": [
    "To conclude, let us write our list of DOIs for non-retracted papers into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "182b7ed6-e37e-4374-9495-c582366a64e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create set with DOIs to get rid of repeated entries\n",
    "\n",
    "final_doi_set = set(final_doi_list)\n",
    "\n",
    "# Convert set to data frame\n",
    "\n",
    "df_unique_dois = pd.DataFrame(final_doi_set)\n",
    "\n",
    "# Write data frame to .csv\n",
    "\n",
    "df_unique_dois.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1e296-4c33-43c8-b04b-b2eb8b4ebb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python \"04c_extract_nonretract_abstract_as_text.py\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
