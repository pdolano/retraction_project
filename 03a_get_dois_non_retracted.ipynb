{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5dd072-bb10-4932-8598-5d3aecba0e48",
   "metadata": {},
   "source": [
    "# 3a. Getting DOIs of Non-Retracted Papers with Specified Country and Year Distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cd73e-5aff-4763-b120-1882a4896458",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af693019-1299-4c1c-a455-28cca2f91a78",
   "metadata": {},
   "source": [
    "This Notebook **obtains DOIs of non-retracted papers**. The papers with these DOIs are **randomly** selected from OpenAlex, and they have the **same country and year distribution** as the retracted papers in our original data set. Needless to say, they will also be papers from the same scientific discipline.\n",
    "\n",
    "The Notebook takes as input the .csv file that was generated by **Notebook 2c**, which contained the country and year distribution of our retracted papers. Its output will in tunr be fed to **Notebook 3b** in order to download bibliographic information from OpenAlex for our sample of non-retracted papers.\n",
    "\n",
    "The **workflow** of the notebook is therefore as follows:\n",
    "\n",
    "- Input: **one .csv file** with the country and year distribution of our retracted papers.\n",
    "- Output: **one .csv file** with the DOIs of non-retracted papers from OpenAlex with the required country and year profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db77eb-9d5d-4ebc-8b74-f27acd4eb7dd",
   "metadata": {},
   "source": [
    "## Input / Output Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c809d-2f22-4af8-a683-9a9685d44889",
   "metadata": {},
   "source": [
    "Input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db710a39-004d-4279-868e-d3469f5048cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# File path for .csv input file with bucketing specifications\n",
    "# Note that the old input path is to a file that we created manually to sidestep problems\n",
    "# Please use the new input path, which contains the actual year and country profile for our retracted papers\n",
    "#input_path_old = \"../data/Country_Year_Buckets_Cellbio.csv\" \n",
    "input_path = \"../data/buckets/cell_biology/cell_bio_buckets_value.csv\"\n",
    "\n",
    "# Id of subfield to fitler our paper search \n",
    "# Id value for cell_bio is 1307\n",
    "subfield_filter_value = \"1307\"\n",
    "\n",
    "# File path for ISO country code equivalences\n",
    "input_path_dictionary = \"../data/country_code_dictionary.csv\"\n",
    "\n",
    "# Upsize factor for each bucket\n",
    "upsize_factor = 1.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effe7e7-ce8f-4521-9936-ab38f8e5cc1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Output parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05e7dbf-cc3e-4f83-8cb6-4081153c4102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# File path for .csv with DOIs of non-retracted papers\n",
    "\n",
    "output_path = \"../data/dois_non_retracted/cell_biology/non_retracted_dois_cell_bio.csv\" \n",
    "\n",
    "# File path for log file\n",
    "\n",
    "output_path_log = \"../data/logs/cell_biology/non_retracted_dois_download.txt\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45085f7-21d4-460c-8d59-fd8a28ac73fb",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989ee18-aab1-4569-b140-f84b86beed73",
   "metadata": {},
   "source": [
    "\n",
    "As always, let's start by importing all required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea4fa866-c999-433d-bb8f-0eabffc89178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from json.decoder import JSONDecodeError\n",
    "import json\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68239fe-4b98-4391-a0f5-23f8347e3baa",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed31b87-d76c-46ae-9bd6-78105f9234ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "And by loading the relevant data from our .csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b908b82-7ce1-448f-a292-a2a4b3521e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load .csv with bucketing specifications into data frame\n",
    "df = pd.read_csv(input_path, encoding='latin-1', sep = \",\") #Note that the separator is \",\" this time around\n",
    "\n",
    "# Load .csv file with ISO country code equivalences\n",
    "df_country_codes = pd.read_csv(\"../data/country_codes_dictionary.csv\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215177a-6c7b-4101-a83c-f96e9c69c4df",
   "metadata": {},
   "source": [
    "The data specifying our contry code equivalences requires some cleaninig, so we will go ahead and make the necessary adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c10835b3-a832-4792-a8df-b0d1987d76db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Clean spurious spaces in \"Country\" column\n",
    "df_country_codes['Country'] = df_country_codes['Country'].str.strip()\n",
    "\n",
    "# Clean spurious spaces in \"TIS\" column\n",
    "df_country_codes['TIS'] = df_country_codes['TIS'].str.strip()\n",
    "\n",
    "# Create country code dictionary from countr code data frame\n",
    "country_codes_dictionary = df_country_codes.set_index('Country')['TIS'].str.strip().to_dict()\n",
    "\n",
    "# Rename country column \n",
    "df.rename(columns={\"ï»¿country\": \"country\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cab970f4-9ec5-4843-90ce-e575d52107c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_country</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AM</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR</td>\n",
       "      <td>2009</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>2011</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>ZA</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>ZA</td>\n",
       "      <td>2012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>ZA</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>ZA</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>ZW</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>801 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_country  publication_year  count\n",
       "0               AM              2019      1\n",
       "1               AR              2009      1\n",
       "2               AR              2010      1\n",
       "3               AR              2011      2\n",
       "4               AR              2013      2\n",
       "..             ...               ...    ...\n",
       "796             ZA              2005      1\n",
       "797             ZA              2012      1\n",
       "798             ZA              2013      2\n",
       "799             ZA              2017      1\n",
       "800             ZW              2007      1\n",
       "\n",
       "[801 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3916d0-ffea-4a49-91a7-de6a6364ce8a",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21699f0e-b65c-4f39-995e-05af825570b0",
   "metadata": {},
   "source": [
    "\n",
    "Our goal in this Notebook will consist of obtaining a list of DOIs for non-retracted papers. So as to avoid biases in our model, we will make sure that these papers belong to the same discipline, and have the same country and year distribution as our retracted papers (recall, of course, that we obtained the country and year \"buckets\" for these papers in a previous Notebook). \n",
    "\n",
    "To do this, we will query OpenAlex for papers with these characteristics. This, in turn, is done by making an API call to an URL address that incorporates the appropriate filters. The first step in our task, therefore, will be to define a function that builds an OpenAlex-compatible URL, given a number of paper characteristics that we want to filter our search for:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f42aeb-139c-4a79-842f-7b34dd22ef82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define url_builder function\n",
    "\n",
    "def url_builder(country, year, field_id, page = \"1\"):\n",
    "    \"\"\"\n",
    "    Builds an URL that can be used to search papers with appropriate filters on OpenAlex \n",
    "    \n",
    "    Parameters: \n",
    "        country (str): country of papers to be queried\n",
    "        year (str): year of papers to be queried\n",
    "        field_id (str): id code of field of papers\n",
    "        page (str): number of page from which papers will be queried (in case we need more papers than can fit in a single \"page\")\n",
    "    \n",
    "    Returns:\n",
    "        url (str): full url to be used for OpenAlex API query\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add subfield filter to our URL\n",
    "    subfield_filter = \"primary_topic.subfield.id:\" + field_id    \n",
    "        \n",
    "    # Add publication year filter to our URL\n",
    "    year_filter = \"publication_year:\" + year\n",
    "\n",
    "    # Add retraction filter to make sure queried papers are NOT retracted\n",
    "    retraction_filter = \"is_retracted:false\"\n",
    "    \n",
    "    # Add country filter to our URL\n",
    "    country_code = country\n",
    "    country_filter = \"institutions.country_code:\" + country_code\n",
    "    \n",
    "    # Add type of work filter\n",
    "    type_filter = \"type:article\"\n",
    "    \n",
    "    # Add page number\n",
    "    page_number = page\n",
    "    \n",
    "    # Add filters to base url\n",
    "    url = \"https://api.openalex.org/works?page=\" + page_number + \"&per-page=200&filter=\" + subfield_filter + \",\" + year_filter + \",\" + country_filter + \",\" + type_filter + \",\" + retraction_filter\n",
    "\n",
    "    # Return full URL\n",
    "    return url\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde007b6-b9fb-4304-9d0d-83ed66c627bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our next step will consist of obtaining a specified number of DOI addresses for the appropriately filtered papers that our URLs can query from OpenAlex. We will define a new function in order to do that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b81958a-d689-4daa-9508-cb515986ea35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Master function\n",
    "# Define doi_getter function\n",
    "\n",
    "def doi_getter(country, year, field, paper_num):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts DOIs of papers based on specified criteria.\n",
    "\n",
    "    Parameters:\n",
    "        country (str): The country associated with the papers.\n",
    "        year (str): The year of publication of the papers.\n",
    "        field (str): The field or discipline of the papers.\n",
    "        paper_num (int): The number of DOIs to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list of str: A list containing the extracted DOIs of the papers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize list to store DOIs\n",
    "    doi_lst = []  \n",
    "    \n",
    "    # Calculate the number of pages require to fetch the specified number of papers\n",
    "    page_num = int(paper_num / 200) + 1\n",
    "    \n",
    "    # Perform API calls until the desired number of DOIs is collected\n",
    "    for page in range(1, page_num + 1):\n",
    "\n",
    "        # Update search URL with page number\n",
    "        page_code = str(page)\n",
    "        url = url_builder(country,year,field,page_code)\n",
    "        \n",
    "        # Perform API call and decode JSON result to obtain meta-data\n",
    "        response = requests.get(url)\n",
    "        meta_data = response.json()\n",
    "                \n",
    "        # Extract DOIs from API response and add to doi_lst\n",
    "        for element in meta_data[\"results\"]:\n",
    "            if element[\"doi\"] is not None:\n",
    "                doi_lst.append(element[\"doi\"])\n",
    "            if len(doi_lst) >= paper_num:\n",
    "                return list(set(doi_lst))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241ec496-387d-452e-81f4-c2cffba6e915",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Note of course that it is possible that our our API call to access information from OpenAlex may fail or that other complications may arise in our attempts to get DOIs for non-retracted papers. In order to get around this problem, we will try to collect more DOIs of non-retracted papers than we need from each country and year \"bucket.\" In fact, we will make this quantity substantially larger so as to make sure that we always get as many DOIs for non-retracted papers as we need. Of course, this means that we will typically get more DOIs than necessary for each country and year bucket. Whenever this happens, we will randomly select as many DOIs as we actually need from the ones that we obtained.\n",
    "\n",
    "We will therefore need a function to pick a specified number of elements of a given list at random, which we can go on to define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "534ea179-7bb3-4a5e-8075-680c93e70df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define pick_random_entries function\n",
    "\n",
    "def pick_random_entries(input_list, number):\n",
    "    \"\"\"\n",
    "    Picks a specified number of random elements from a list\n",
    "    \n",
    "    Parameters:\n",
    "        input_list (list): The list with elements to be picked at random\n",
    "        number (int): Number of elements to be picked at random\n",
    "    \n",
    "    Returns:\n",
    "        (list): List with specified number of elements picked at random\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate n random indices within the range of the string length\n",
    "    random_indices = random.sample(range(len(input_list)), number)\n",
    "    \n",
    "    # Select the elements at the random indices\n",
    "    random_elements = [input_list[i] for i in random_indices]\n",
    "    \n",
    "    return random_elements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94a2671-bf2f-4247-9d1b-ad93298d3a60",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "Finally, and since we will be obtaining DOIs for a considerable number of papers, it will  be useful to have a sense of how each attempted at downloading information takes. In order to be able to present that information in a more readable format, we will define a function that converts an amount of time expressed in seconds, to the same amount expressed in hours, minutes, and seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5359f6a-e317-40d8-bdec-040a93684803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define seconds_to_hms function\n",
    "\n",
    "def seconds_to_hms(seconds):\n",
    "    \"\"\"\n",
    "    Convert seconds to hours, minutes, and seconds.\n",
    "\n",
    "    Parameters:\n",
    "        seconds (int): Number of seconds.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the equivalent time in hours, minutes, and seconds.\n",
    "    \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "\n",
    "    return hours, minutes, seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c274b-0d80-41a4-9eb4-d23a1036b9e6",
   "metadata": {},
   "source": [
    "## Getting DOIs of Non-Retracted Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0975fc6f-dc1d-4863-b45a-f8f370928abf",
   "metadata": {},
   "source": [
    "We can now make use of these functions to obtain the desired number of DOIs for non-retracted papers for each year and country. We will also write a log documenting how the download process advances and how many DOIs are obtained for each year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d793f24-454c-4a6d-8e9d-0fb67104b1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m triple_count = \u001b[32m3\u001b[39m*count\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Call doi_getter to get three times as many DOI URLs as retraced papers from that bucket\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m doi_list = \u001b[43mdoi_getter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m1307\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriple_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Ensure doi_list is not None to avoid errors\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m doi_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mdoi_getter\u001b[39m\u001b[34m(country, year, field, paper_num)\u001b[39m\n\u001b[32m     30\u001b[39m url = url_builder(country,year,field,page_code)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Perform API call and decode JSON result to obtain meta-data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m meta_data = response.json()\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Extract DOIs from API response and add to doi_lst\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/retraction_project/.venv/lib/python3.13/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/retraction_project/.venv/lib/python3.13/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/retraction_project/.venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/retraction_project/.venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/retraction_project/.venv/lib/python3.13/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/retraction_project/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/retraction_project/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/retraction_project/.venv/lib/python3.13/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1430\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1428\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1430\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1431\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1432\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Open log file\n",
    "with open(\"output_path_log\", \"w\") as f:\n",
    "\n",
    "    # Write introductory message into log\n",
    "    f.write(\"Log file opened.\\n\")\n",
    "    \n",
    "    # Store start time of loop execution\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize list to store dois\n",
    "    final_doi_list = []\n",
    "\n",
    "    # For loop to gather dois\n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        # Intialize variables with values from input bucketing specifications\n",
    "        year = str(row['publication_year'])\n",
    "        country = str(row['author_country'])\n",
    "        count = row['count']\n",
    "        triple_count = 3*count\n",
    "\n",
    "        # Call doi_getter to get three times as many DOI URLs as retraced papers from that bucket\n",
    "        doi_list = doi_getter(country, year, \"1307\", triple_count)\n",
    "\n",
    "        # Ensure doi_list is not None to avoid errors\n",
    "        if doi_list is None:\n",
    "            doi_list = []\n",
    "    \n",
    "        # Write log entry with number of DOIs grabbed for this iteration of the loop\n",
    "        f.write(\"+++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        f.write(f\"LOOP NUMBER {index + 1}: Year={year}, COUNTRY={country}, COUNT={count}. \\n\")\n",
    "        f.write(f\"Picked {len(doi_list)} DOIs, triple count was {triple_count}. \\n\")\n",
    "        f.write(f\"Of those, {len(set(doi_list))} were unique DOIs \\n\\n\")\n",
    "\n",
    "        # Obtain size of upsized sample from upsize factor defined at the beginning\n",
    "        upsized_sample = int(count * upsize_factor)\n",
    "    \n",
    "        # Randomly get a number of dois equal to a slightly upsized sample size\n",
    "        if len(doi_list) > 0 and len(doi_list) > upsized_sample:\n",
    "            doi_list = pick_random_entries(doi_list, upsized_sample)\n",
    "        else:\n",
    "            doi_list = []\n",
    "        \n",
    "        # Writing log entry with number of DOIs randomly grabbed at this point\n",
    "        f.write(f\"I then picked {len(doi_list)} DOIs randomly. \\n\")\n",
    "        f.write(f\"Of these, {len(set(doi_list))} were unique DOIs. \\n\")\n",
    "        f.write(f\"Target count was {count}.\\n\")\n",
    "        \n",
    "        if len(doi_list) != len(set(doi_list)):\n",
    "            f.write(f\"ERROR: WE HAVE {len(doi_list)} - {len(set(doi_list))} REPEATED DOIS. \\n\")\n",
    "        \n",
    "        # Add result of current iteration to final list of dois\n",
    "        final_doi_list.extend(doi_list) \n",
    "\n",
    "    # Calculate elapsed time for for lopp execution\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_hours, elapsed_minutes, elapsed_seconds = seconds_to_hms(elapsed_time)\n",
    "\n",
    "    print(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {round(elapsed_seconds,1)}s. \\n\")\n",
    "\n",
    "    # Write final message in log file\n",
    "    f.write(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {elapsed_seconds}s. \\n\")\n",
    "    f.write(\"End of log file.\\n\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d339da5-1ae5-4f3c-9268-b05ebcc8e439",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c4dd3-835a-4b34-8324-fa2feeb4624b",
   "metadata": {},
   "source": [
    "To conclude, let us write our list of DOIs for non-retracted papers into a .csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "182b7ed6-e37e-4374-9495-c582366a64e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create set with DOIs to get rid of repeated entries\n",
    "\n",
    "final_doi_set = set(final_doi_list)\n",
    "\n",
    "# Convert set to data frame\n",
    "\n",
    "df_unique_dois = pd.DataFrame(final_doi_set)\n",
    "\n",
    "# Write data frame to .csv\n",
    "\n",
    "df_unique_dois.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb1e296-4c33-43c8-b04b-b2eb8b4ebb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python \"04c_extract_nonretract_abstract_as_text.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "966bc7ca-3dae-4b87-b961-a2e85c0b3b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6581"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_unique_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0ba13-da18-4f41-b261-388359f29a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
