{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5dd072-bb10-4932-8598-5d3aecba0e48",
   "metadata": {},
   "source": [
    "# 4a. Fetching DOIs for Non-Fraudulent Papers with .csv File with Bucketing Specifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425cd73e-5aff-4763-b120-1882a4896458",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af693019-1299-4c1c-a455-28cca2f91a78",
   "metadata": {},
   "source": [
    "This Notebook **obtains the DOIs of a number of randomly selected non-retracted papers, whose year and country specifications match those of the retracted papers under investigation**. In order to do that, we will pick the DOIs for our non-retracted papers from a number of \"buckets\" that we will define for each year and country. These buckest will be built so as to contain as many non-retracted papers as retracted papers we had in our original data base, for each year and country. \n",
    "\n",
    "The Notebook takes as input the .csv file that was generated by **script 3b**, which contained the size of the bucket associated to each year and country. \n",
    "\n",
    "The **workflow** of the notebook is therefore as follows:\n",
    "\n",
    "- Input: **two .csv files**. The first one contains the bucketing specifications obtained from script 3b, whereas the second one gives us the appropriate code for each country, which we will use to add the appropriate filters to our search.\n",
    "\n",
    "- Output: **one .csv file** with the DOIs of non-retracted papers gathered from our buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2db77eb-9d5d-4ebc-8b74-f27acd4eb7dd",
   "metadata": {},
   "source": [
    "## Input / Output Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475c809d-2f22-4af8-a683-9a9685d44889",
   "metadata": {},
   "source": [
    "- Input parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db710a39-004d-4279-868e-d3469f5048cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# File path for .csv input file with bucketing specifications\n",
    "\n",
    "input_path = \"../data/Country_Year_Buckets_Cellbio.csv\"\n",
    "\n",
    "# Id of subfield to fitler our paper search \n",
    "# Id value for cell_bio is 1307\n",
    "\n",
    "subfield_filter_value = \"1307\"\n",
    "\n",
    "# File path for ISO country code equivalences\n",
    "\n",
    "input_path_dictionary = \"../data/country_code_dictionary.csv\"\n",
    "\n",
    "# Upsize factor for each bucket\n",
    "\n",
    "upsize_factor = 1.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3effe7e7-ce8f-4521-9936-ab38f8e5cc1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Output parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05e7dbf-cc3e-4f83-8cb6-4081153c4102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# File name for .csv with DOIs of non-retracted papers\n",
    "\n",
    "output_file_name = \"dois_jenny_corrected_3.csv\"\n",
    "\n",
    "# File name for .csv with unique DOIs of non-retracted papers\n",
    "\n",
    "output_path_unique = \"../data/results/dois_jenny_unique.csv\"\n",
    "\n",
    "# File path for .jsonl file with text data for abstracts\n",
    "\n",
    "output_path = \"../data/results/\" + output_file_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45085f7-21d4-460c-8d59-fd8a28ac73fb",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989ee18-aab1-4569-b140-f84b86beed73",
   "metadata": {},
   "source": [
    "\n",
    "- As always, let's start by importing all required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea4fa866-c999-433d-bb8f-0eabffc89178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from json.decoder import JSONDecodeError\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68239fe-4b98-4391-a0f5-23f8347e3baa",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed31b87-d76c-46ae-9bd6-78105f9234ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "- And by loading the relevant data from our .csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b908b82-7ce1-448f-a292-a2a4b3521e10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load .csv with bucketing specifications into data frame\n",
    "\n",
    "df = pd.read_csv(input_path, encoding='latin-1', sep = \";\")\n",
    "\n",
    "# Load .csv file with ISO country code equivalences\n",
    "\n",
    "df_country_codes = pd.read_csv(\"../data/country_codes_dictionary.csv\", encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d215177a-6c7b-4101-a83c-f96e9c69c4df",
   "metadata": {},
   "source": [
    "- The data specifying our contry code equivalences requires some cleaninig, so we will go ahead and make the necessary adjustments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10835b3-a832-4792-a8df-b0d1987d76db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Clean spurious spaces in \"Country\" column\n",
    "\n",
    "df_country_codes['Country'] = df_country_codes['Country'].str.strip()\n",
    "\n",
    "# Clean spurious spaces in \"TIS\" column\n",
    "\n",
    "df_country_codes['TIS'] = df_country_codes['TIS'].str.strip()\n",
    "\n",
    "# Create country code dictionary from countr code data frame\n",
    "\n",
    "country_codes_dictionary = df_country_codes.set_index('Country')['TIS'].str.strip().to_dict()\n",
    "\n",
    "# Rename country column \n",
    "\n",
    "df.rename(columns={\"ï»¿country\": \"country\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3916d0-ffea-4a49-91a7-de6a6364ce8a",
   "metadata": {},
   "source": [
    "## Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5359f6a-e317-40d8-bdec-040a93684803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seconds_to_hms(seconds):\n",
    "    \"\"\"\n",
    "    Convert seconds to hours, minutes, and seconds.\n",
    "\n",
    "    Parameters:\n",
    "    seconds (int): Number of seconds.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the equivalent time in hours, minutes, and seconds.\n",
    "    \"\"\"\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "\n",
    "    return hours, minutes, seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f42aeb-139c-4a79-842f-7b34dd22ef82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def url_builder(country, year, field_id, page = \"1\"):\n",
    "    \"\"\"This function builds the URL that OpenAlex requires to find papers with the characteristics\n",
    "    specified in the filters below.\"\"\"\n",
    "    \n",
    "    # Add subfield filter to our URL\n",
    "    \n",
    "    subfield_filter = \"primary_topic.subfield.id:\" + field_id\n",
    "    \n",
    "    # Call country_code_getter to obtain code for year\n",
    "    \n",
    "    country_code = country\n",
    "        \n",
    "    # Add publication year filter to our URL\n",
    "    \n",
    "    year_filter = \"publication_year:\" + year\n",
    "\n",
    "    # Add retraction filter\n",
    "\n",
    "    retraction_filter = \"is_retracted:false\"\n",
    "    \n",
    "    # Add country filter to our URL\n",
    "    \n",
    "    country_filter = \"institutions.country_code:\" + country_code\n",
    "    \n",
    "    # Add type of work giler\n",
    "    \n",
    "    type_filter = \"type:article\"\n",
    "    \n",
    "    # Add page number\n",
    "    \n",
    "    page_number = page\n",
    "    \n",
    "    # Add filters to base url\n",
    "    \n",
    "    url = \"https://api.openalex.org/works?page=\" + page_number + \"&per-page=200&filter=\" + subfield_filter + \",\" + year_filter + \",\" + country_filter + \",\" + type_filter + \",\" + retraction_filter\n",
    "\n",
    "    # Return full URL\n",
    "    \n",
    "    return url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b81958a-d689-4daa-9508-cb515986ea35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def doi_getter_pagination(country, year, field, paper_num):\n",
    "    \n",
    "    \"\"\"Function takes the URL to perform an API search in OpenAlex with a number of \n",
    "    search filters. It then extracts the DOIs of the specified number of papers, then \n",
    "    returns a list with the DOIs in question\"\"\"\n",
    "\n",
    "    doi_lst = []  # Initialize list to store DOIs\n",
    "    page = 1  # Initialize page number\n",
    "    \n",
    "    # Calculate required page number\n",
    "    \n",
    "    page_num = int(paper_num / 200) + 1\n",
    "    \n",
    "    # Perform API calls until the desired number of DOIs is collected\n",
    "    \n",
    "    for page in range(1, page_num + 1):\n",
    "\n",
    "        # Update search URL with page number\n",
    "        \n",
    "        page_code = str(page)\n",
    "        url = url_builder(country,year,field,page_code)\n",
    "        \n",
    "        # Perform API call and decode JSON result to obtain meta-data\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        meta_data = response.json()\n",
    "        \n",
    "        # Extract DOIs from API response and add to doi_lst\n",
    "        \n",
    "        # Extract DOIs from API response and add to doi_lst\n",
    "        \n",
    "        for element in meta_data[\"results\"]:\n",
    "            if element[\"doi\"] is not None:\n",
    "                doi_lst.append(element[\"doi\"])\n",
    "            if len(doi_lst) >= paper_num:\n",
    "                return list(set(doi_lst))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6048f298-2050-4144-8173-2c2b179c1088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def doi_from_address(address_lst):\n",
    "    \"\"\"Function takes the whole URL address associated to the DOI of a given paper, \n",
    "    then subtracts the first part of the address to obtain its DOI code only. It\n",
    "    takes as input a list of DOI URLs, returns a list of DOI codes\"\"\"\n",
    "\n",
    "    doi_lst = []\n",
    "    \n",
    "    for element in address_lst:\n",
    "        doi_lst.append(element.removeprefix(\"https://doi.org/\"))\n",
    "        \n",
    "    return doi_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989b0eea-ae85-4c20-9fce-52082447d21f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def address_builder(doi):\n",
    "    \"\"\"Takes a DOI identifier and builds full URL address from it, with format\n",
    "    required for a normal OpenAlex API call\"\"\"\n",
    "    \n",
    "    # Store url addresses in string\n",
    "    \n",
    "    base_address = \"https://api.openalex.org/works/https://doi.org/\" + doi\n",
    "    polite_address = base_address + \"?mailto=\" + \"pabloruizdeolano@gmail.com\" # Use polite address for faster API call performance\n",
    "    \n",
    "    # Return polite address\n",
    "    \n",
    "    return polite_address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c88a570-4b3a-40a5-8aa6-4a85833b180f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def meta_data_extractor(doi, n_grams=\"False\"):\n",
    "    \"\"\"Function takes a DOI and calls address_builder function to build full URL. \n",
    "    It then performs an API call, and it returns the metadata as JSON dictionary.\n",
    "    If n_grams is set to \"True\" it grabs the n_grams for the paper, if those \n",
    "    are avaiable. Otherwise it grabs its abstract word index.\"\"\"\n",
    "    \n",
    "    # Build correct address depending on value of \"n_grams\" parameter\n",
    "    \n",
    "    if n_grams == \"False\":\n",
    "        url = address_builder(doi)\n",
    "    else:\n",
    "        url = address_builder_ngrams(doi)\n",
    "    \n",
    "    # Perform API call and store result in response variable\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Convert meta-data to json format if possible and store result in variable\n",
    "    \n",
    "    try:\n",
    "        meta_data = response.json()\n",
    "    except JSONDecodeError: # This is in case result of API call was not in JSON format\n",
    "        print(\"Error: Unable to decode JSON response\")\n",
    "        meta_data = None\n",
    "    \n",
    "    return meta_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "534ea179-7bb3-4a5e-8075-680c93e70df9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def pick_random_entries(string, n):\n",
    "    \n",
    "    # Generate n random indices within the range of the string length\n",
    "    \n",
    "    random_indices = random.sample(range(len(string)), n)\n",
    "    \n",
    "    # Select the characters at the random indices\n",
    "    \n",
    "    random_entries = [string[i] for i in random_indices]\n",
    "    \n",
    "    return random_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c274b-0d80-41a4-9eb4-d23a1036b9e6",
   "metadata": {},
   "source": [
    "## Getting DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "948d12c1-c713-4632-bcc6-22f59192f436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for the loop: 0.0h, 17.0, 30.622215032577515s. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Open log file\n",
    "\n",
    "with open(\"../data/logs/\" + output_file_name + \"logfile.txt\", \"w\") as f:\n",
    "\n",
    "    f.write(\"Log file opened.\\n\")\n",
    "    \n",
    "    # Store start time of loop execution\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize list to store dois\n",
    "\n",
    "    final_doi_list = []\n",
    "\n",
    "    # For loop to gather dois\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "    \n",
    "        # Intialize variables\n",
    "    \n",
    "        year = str(row['year'])\n",
    "    \n",
    "        country = str(row['country'])\n",
    "    \n",
    "        count = row['count']\n",
    "    \n",
    "        triple_count = 3*count\n",
    "\n",
    "        # Call doi_getter to get list as many filtered DOI URLs as our sample size\n",
    "            \n",
    "        doi_list = doi_getter_pagination(country, year, \"1307\", triple_count)\n",
    "    \n",
    "        # Write log entry with number of DOIs grabbed for this iteration of the loop\n",
    "    \n",
    "        f.write(\"+++++++++++++++++++++++++++++++++++++++++++\")\n",
    "        f.write(f\"LOOP NUMBER {index + 1}: Year={year}, COUNTRY={country}, COUNT={count}. \\n\")\n",
    "        f.write(f\"Picked {len(doi_list)} DOIs, triple count was {triple_count}. \\n\")\n",
    "        f.write(f\"Of those, {len(set(doi_list))} were unique DOIs \\n\\n\")\n",
    "\n",
    "        # Obtain size of upsized sample from upsize factor defined at the beginning\n",
    "    \n",
    "        upsized_sample = int(count * upsize_factor)\n",
    "    \n",
    "        # Randomly get a number of dois equal to a slightly upsized sample size\n",
    "    \n",
    "        if len(doi_list) > 0 and len(doi_list) > upsized_sample:\n",
    "            doi_list = pick_random_entries(doi_list, upsized_sample)\n",
    "        else:\n",
    "            doi_list = []\n",
    "        \n",
    "        # Writing log entry with number of DOIs randomly grabbed at this point\n",
    "    \n",
    "        f.write(f\"I then picked {len(doi_list)} DOIs randomly. \\n\")\n",
    "        f.write(f\"Of these, {len(set(doi_list))} were unique DOIs. \\n\")\n",
    "        f.write(f\"Target count was {count}.\\n\")\n",
    "        \n",
    "        if len(set(doi_list)) != len(set(doi_list)):\n",
    "            f.write(f\"ERROR: WE HAVE {len(doi_list)} - {len(set(doi_list))} REPEATED DOIS. \\n\")\n",
    "        \n",
    "        # Add result of current iteration to final list of dois\n",
    "    \n",
    "        final_doi_list.extend(doi_list) \n",
    "\n",
    "    # Calculate elapsed time for for lopp execution\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    elapsed_hours, elapsed_minutes, elapsed_seconds = seconds_to_hms(elapsed_time)\n",
    "\n",
    "    print(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {round(elapsed_seconds,1)}s. \\n\")\n",
    "\n",
    "    # Write final message in log file\n",
    "    \n",
    "    f.write(f\"Time taken for the loop: {elapsed_hours}h, {elapsed_minutes}m, {elapsed_seconds}s. \\n\")\n",
    "    f.write(\"End of log file.\\n\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d339da5-1ae5-4f3c-9268-b05ebcc8e439",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fe49e7f-c9bb-4451-98eb-f4d6f4fbd7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert list to data frame\n",
    "\n",
    "df_dois = pd.DataFrame(final_doi_list)\n",
    "\n",
    "# Write data frame to .csv\n",
    "\n",
    "df_dois.to_csv(output_path, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "182b7ed6-e37e-4374-9495-c582366a64e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create set with DOIs to get rid of repeated entries\n",
    "\n",
    "final_doi_set = set(final_doi_list)\n",
    "\n",
    "# Convert set to data frame\n",
    "\n",
    "df_unique_dois = pd.DataFrame(final_doi_set)\n",
    "\n",
    "# Write data frame to .csv\n",
    "\n",
    "df_unique_dois.to_csv(output_path_unique, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bb1e296-4c33-43c8-b04b-b2eb8b4ebb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200 files\n",
      "Processed 400 files\n",
      "Processed 600 files\n",
      "Processed 800 files\n",
      "Processed 1000 files\n",
      "Processed 1400 files\n",
      "Processed 1600 files\n",
      "Processed 2200 files\n",
      "Processed 2400 files\n",
      "Processed 2800 files\n",
      "Processed 3200 files\n",
      "Processed 3400 files\n",
      "Processed 3600 files\n",
      "Processed 3800 files\n",
      "Processed 4000 files\n",
      "Processed 4200 files\n",
      "Processed 4400 files\n",
      "Processed 4600 files\n",
      "Processed 4800 files\n",
      "Processed 5000 files\n",
      "Processed 5200 files\n",
      "Number of rows to be written to CSV: 5063\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "!python \"04c_extract_nonretract_abstract_as_text.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9dd95-6a11-45cc-a290-9c42cf946ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
